<!DOCTYPE HTML>
<html lang="en" class="sidebar-visible no-js light">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>Vollo SDK User Guide</title>
        <meta name="robots" content="noindex" />


        <!-- Custom HTML head -->
        
        <meta content="text/html; charset=utf-8" http-equiv="Content-Type">
        <meta name="description" content="">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff" />

        <link rel="shortcut icon" href="assets/vollo-favicon.png">
        <link rel="stylesheet" href="css/variables.css">
        <link rel="stylesheet" href="css/general.css">
        <link rel="stylesheet" href="css/chrome.css">
        <link rel="stylesheet" href="css/print.css" media="print">

        <!-- Fonts -->
        <link rel="stylesheet" href="FontAwesome/css/font-awesome.css">
        <link rel="stylesheet" href="fonts/fonts.css">

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" href="highlight.css">
        <link rel="stylesheet" href="tomorrow-night.css">
        <link rel="stylesheet" href="ayu-highlight.css">

        <!-- Custom theme stylesheets -->

    </head>
    <body>
        <!-- Provide site root to javascript -->
        <script type="text/javascript">
            var path_to_root = "";
            var default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? "navy" : "light";
        </script>

        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script type="text/javascript">
            try {
                var theme = localStorage.getItem('mdbook-theme');
                var sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script type="text/javascript">
            var theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            var html = document.querySelector('html');
            html.classList.remove('no-js')
            html.classList.remove('light')
            html.classList.add(theme);
            html.classList.add('js');
        </script>

        <!-- Hide / unhide sidebar before it is displayed -->
        <script type="text/javascript">
            var html = document.querySelector('html');
            var sidebar = 'hidden';
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            }
            html.classList.remove('sidebar-visible');
            html.classList.add("sidebar-" + sidebar);
        </script>

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <div class="sidebar-scrollbox">
                <a href="./">
                  <img class="sidebar-logo">
                  </img>
                </a>

                <ol class="chapter"><li class="chapter-item expanded affix "><a href="introduction.html">Introduction</a></li><li class="chapter-item expanded "><a href="installation.html"><strong aria-hidden="true">1.</strong> Installation</a></li><li class="chapter-item expanded "><a href="key-features.html"><strong aria-hidden="true">2.</strong> Key Features</a></li><li class="chapter-item expanded "><a href="getting-started.html"><strong aria-hidden="true">3.</strong> Getting Started</a></li><li class="chapter-item expanded "><a href="vollo-compiler.html"><strong aria-hidden="true">4.</strong> Vollo Compiler</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="supported-models.html"><strong aria-hidden="true">4.1.</strong> Supported Models</a></li><li class="chapter-item expanded "><a href="example-1-mlp.html"><strong aria-hidden="true">4.2.</strong> Example 1: MLP</a></li><li class="chapter-item expanded "><a href="example-2-cnn.html"><strong aria-hidden="true">4.3.</strong> Example 2: CNN</a></li><li class="chapter-item expanded "><a href="example-3-multi-model.html"><strong aria-hidden="true">4.4.</strong> Example 3: Multiple Models in a Vollo Program</a></li><li class="chapter-item expanded "><a href="vollo-onnx.html"><strong aria-hidden="true">4.5.</strong> ONNX Support</a></li></ol></li><li class="chapter-item expanded "><a href="benchmark.html"><strong aria-hidden="true">5.</strong> Benchmarks</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="benchmark-mlp.html"><strong aria-hidden="true">5.1.</strong> MLP</a></li><li class="chapter-item expanded "><a href="benchmark-cnn.html"><strong aria-hidden="true">5.2.</strong> CNN</a></li><li class="chapter-item expanded "><a href="benchmark-lstm.html"><strong aria-hidden="true">5.3.</strong> LSTM</a></li><li class="chapter-item expanded "><a href="benchmark-io.html"><strong aria-hidden="true">5.4.</strong> IO Round Trip</a></li></ol></li><li class="chapter-item expanded "><a href="accelerator-setup.html"><strong aria-hidden="true">6.</strong> Accelerator Setup</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="system-requirements.html"><strong aria-hidden="true">6.1.</strong> System Requirements</a></li><li class="chapter-item expanded "><a href="programming-the-fpga.html"><strong aria-hidden="true">6.2.</strong> Programming the FPGA</a></li><li class="chapter-item expanded "><a href="licensing.html"><strong aria-hidden="true">6.3.</strong> Licensing</a></li><li class="chapter-item expanded "><a href="running-an-example.html"><strong aria-hidden="true">6.4.</strong> Running an Example</a></li><li class="chapter-item expanded "><a href="running-the-benchmark.html"><strong aria-hidden="true">6.5.</strong> Running the Benchmark</a></li></ol></li><li class="chapter-item expanded "><a href="vollo-runtime.html"><strong aria-hidden="true">7.</strong> Vollo Runtime</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="c-api.html"><strong aria-hidden="true">7.1.</strong> C API</a></li><li class="chapter-item expanded "><a href="vollo-rt-example.html"><strong aria-hidden="true">7.2.</strong> C Example</a></li><li class="chapter-item expanded "><a href="vollo-rt-python-example.html"><strong aria-hidden="true">7.3.</strong> Python Example</a></li></ol></li><li class="chapter-item expanded "><a href="ip-core/0-intro.html"><strong aria-hidden="true">8.</strong> Vollo IP Core</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="ip-core/1-selecting-an-ip-core.html"><strong aria-hidden="true">8.1.</strong> Selecting an IP Core</a></li><li class="chapter-item expanded "><a href="ip-core/2-interface.html"><strong aria-hidden="true">8.2.</strong> IP Core Interface</a></li><li class="chapter-item expanded "><a href="ip-core/3-quartus-integration.html"><strong aria-hidden="true">8.3.</strong> Quartus Integration</a></li><li class="chapter-item expanded "><a href="ip-core/4-config.html"><strong aria-hidden="true">8.4.</strong> Runtime configuration</a></li><li class="chapter-item expanded "><a href="ip-core/5-example-design.html"><strong aria-hidden="true">8.5.</strong> Example design</a></li></ol></li><li class="chapter-item expanded "><a href="versions.html"><strong aria-hidden="true">9.</strong> Versions</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="release-notes.html"><strong aria-hidden="true">9.1.</strong> Release Notes</a></li></ol></li></ol>
            </div>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle"></div>
        </nav>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                                <div id="menu-bar-hover-placeholder"></div>
                <div id="menu-bar" class="menu-bar sticky bordered">
                    <div class="left-buttons">
                        <button id="sidebar-toggle" class="icon-button" type="button" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                            <i class="fa fa-bars"></i>
                        </button>
                        <button id="theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="theme-list">
                            <i class="fa fa-paint-brush"></i>
                        </button>
                        <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="light">Light (default)</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="ayu">Ayu</button></li>
                        </ul>
                        <button id="search-toggle" class="icon-button" type="button" title="Search. (Shortkey: s)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="S" aria-controls="searchbar">
                            <i class="fa fa-search"></i>
                        </button>
                    </div>

                    <h1 class="menu-title">Vollo SDK User Guide</h1>

                    <div class="right-buttons">
                        <a href="print.html" title="Print this book" aria-label="Print this book">
                            <i id="print-button" class="fa fa-print"></i>
                        </a>

                    </div>
                </div>

                <div id="search-wrapper" class="hidden">
                    <form id="searchbar-outer" class="searchbar-outer">
                        <input type="search" id="searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="searchresults-outer" aria-describedby="searchresults-header">
                    </form>
                    <div id="searchresults-outer" class="searchresults-outer hidden">
                        <div id="searchresults-header" class="searchresults-header"></div>
                        <ul id="searchresults">
                        </ul>
                    </div>
                </div>

                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script type="text/javascript">
                    document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="content" class="content">
                    <main>
                        <h1 id="introduction"><a class="header" href="#introduction">Introduction</a></h1>
<p>The <a href="https://github.com/MyrtleSoftware/vollo-sdk">Vollo SDK</a> is designed for
low latency streaming inference of machine learning (ML) models on FPGA
platforms.</p>
<p>You can estimate the latency of your model using the Vollo SDK without needing
an FPGA or a Vollo license, see <a href="getting-started.html">Getting Started</a> for
details.</p>
<p>This document outlines the following:</p>
<ul>
<li><a href="installation.html">Installation</a></li>
<li><a href="key-features.html">Key features of Vollo</a></li>
<li><a href="getting-started.html">Steps to get started with Vollo</a></li>
<li>The <a href="vollo-compiler.html">Vollo Compiler API</a></li>
<li>The <a href="vollo-runtime.html">Vollo Runtime API</a></li>
<li><a href="accelerator-setup.html">Hardware requirements and setup</a></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="installation"><a class="header" href="#installation">Installation</a></h1>
<p>The latest SDK is available for download from <a href="https://github.com/MyrtleSoftware/vollo-sdk/releases">https://github.com/MyrtleSoftware/vollo-sdk/releases</a>.</p>
<p>Download the <code>vollo-sdk-&lt;version&gt;.run</code> self-extractable archive and execute it
to extract the Vollo SDK contents to the current directory.</p>
<pre><code class="language-sh">chmod +x vollo-sdk-&lt;version&gt;.run
./vollo-sdk-&lt;version&gt;.run
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="key-features"><a class="header" href="#key-features">Key Features</a></h1>
<p>Vollo accelerates machine learning inference for low latency streaming models
typically found in financial trading or fraud detection systems such as:</p>
<ul>
<li>Market predictions</li>
<li>Risk analysis</li>
<li>Anomaly detection</li>
<li>Portfolio optimisation</li>
</ul>
<p>Vollo is able to process of range of models, including models which maintain
state while streaming such as convolutional models.</p>
<p>Key characteristics of Vollo are:</p>
<ul>
<li>Low latency inference of machine learning models, typically between 5-10μs.</li>
<li>High accuracy inference through use of Brain Floating Point 16 (bfloat16)
numerical format.</li>
<li>High density processing in a 1U server form factor suitable for co-located
server deployment.</li>
<li>Compiles a range of PyTorch models for use on the accelerator.</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="getting-started"><a class="header" href="#getting-started">Getting Started</a></h1>
<p>You can get started with evaluating your ML model's performance on Vollo using
the Vollo compiler and Vollo virtual machine (VM), which don't require an FPGA
accelerator.</p>
<p>When you are ready, you can run inferences with your model on a Vollo FPGA
accelerator using an evaluation license.</p>
<h2 id="performance-estimation-and-model-design-with-the-vollo-compiler"><a class="header" href="#performance-estimation-and-model-design-with-the-vollo-compiler">Performance estimation and model design with the Vollo compiler</a></h2>
<p>You can use the Vollo compiler and VM to compile and estimate the performance of
your model in an ML user's environment without any accelerator.</p>
<p><img src="assets/evaluation-flow-cpu.svg" alt="CPU evaluation flow" /></p>
<p>The Vollo compiler and Vollo VM execution time is typically on the order of
seconds, enabling fast model iteration for tuning models to meet a latency
target.</p>
<p>To estimate performance of your model with the Vollo SDK:</p>
<ol>
<li>
<p><a href="installation.html">Download and extract</a> the Vollo SDK.</p>
</li>
<li>
<p><a href="vollo-compiler.html#installation">Install the Vollo compiler</a> Python libraries.</p>
</li>
<li>
<p>Compile your model using the Vollo compiler and run the compiled model in the
Vollo VM to generate a compute latency estimate
that will be achieved with Vollo.</p>
<p>See Vollo compiler <a href="example-1-mlp.html">Example 1</a> for a fully worked example
of this including performance estimation.</p>
</li>
<li>
<p>Add in <a href="benchmark-io.html">IO latency</a> for your model characteristics in
order to estimate end to end latency.</p>
</li>
<li>
<p>Iterate on your model architecture to meet your combined latency and accuracy
requirements.</p>
</li>
</ol>
<h2 id="validating-inference-performance-using-the-vollo-fpga-accelerator"><a class="header" href="#validating-inference-performance-using-the-vollo-fpga-accelerator">Validating inference performance using the Vollo FPGA accelerator</a></h2>
<p>When you are ready to run inferences with your models on a Vollo accelerator,
you will need a <a href="system-requirements.html#accelerator-card-requirements">compatible FPGA based PCIe accelerator
card</a> and a <a href="licensing.html">Vollo
license</a>.</p>
<p>Evaluation licenses can be provided free of charge by contacting
<a href="mailto:vollo@myrtle.ai">vollo@myrtle.ai</a>.</p>
<p><img src="assets/evaluation-flow-fpga.svg" alt="FPGA evaluation flow" /></p>
<p>To validate inference performance on Vollo:</p>
<!-- markdownlint-disable MD029 -->
<ol start="6">
<li>
<p>Follow the steps to <a href="programming-the-fpga.html">program</a> and
<a href="licensing.html">license</a> the FPGA.</p>
</li>
<li>
<p>Compile your model and save it as a <code>.vollo</code> program file using the Vollo
compiler.</p>
<p>See Vollo compiler <a href="example-1-mlp.html">Example 1</a> for a fully worked example.</p>
</li>
<li>
<p>Run and benchmark your model on the accelerator using <a href="running-an-example.html">the Vollo runtime C
example</a>.</p>
<p>Make sure to pass the example application the path to your saved <code>.vollo</code>
program when you invoke it on the command line.</p>
</li>
</ol>
<!-- markdownlint-enable MD029 -->
<p>Note that the Vollo SDK includes prebuilt FPGA bitstreams for selected PCIe
accelerator cards so no FPGA compilation or configuration is required after
initial accelerator setup.
As a result loading user models to run on Vollo takes under a second, enabling
fast onboard iteration and evaluation of different models.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="vollo-compiler"><a class="header" href="#vollo-compiler">Vollo Compiler</a></h1>
<p>The Vollo compiler is made up of 2 Python libraries:</p>
<ul>
<li>The <code>vollo-torch</code> PyTorch frontend to the compiler.</li>
<li>The <code>vollo-compiler</code> backend that can transform and compile a model to a
Vollo program (<code>.vollo</code> file).</li>
</ul>
<p>The <a href="vollo-runtime.html">Vollo Runtime</a> section describes how to run a Vollo
program on a Vollo accelerator.
The Vollo compiler API also includes functionality to simulate and estimate
performance of Vollo programs.</p>
<h2 id="api-reference"><a class="header" href="#api-reference">API Reference</a></h2>
<p>This chapter walks through examples of how to use the Vollo compiler that
should cover the most commonly used parts of the API.</p>
<!-- markdown-link-check-disable -->
<p>A more complete API reference can be found <a href="./api-reference">here</a>.</p>
<!-- markdown-link-check-enable -->
<h2 id="installation-1"><a class="header" href="#installation-1">Installation</a></h2>
<p>Set up Vollo environment variables by <a href="accelerator-setup.html#environment-variable-setup">sourcing
<code>setup.sh</code></a> in <code>bash</code>.</p>
<p>Install the wheel files for the Vollo compiler libraries. It's recommended that
you install these into a <a href="https://docs.python.org/3/library/venv.html">virtual
environment</a>.</p>
<p>Note: the packaged wheels only support python 3.7 or greater</p>
<pre><code class="language-sh">python3 -m venv vollo-venv
source vollo-venv/bin/activate
pip install --upgrade pip
pip install "$VOLLO_SDK"/python/*.whl
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="supported-models"><a class="header" href="#supported-models">Supported Models</a></h1>
<p>The Vollo compiler supports PyTorch models that use the following operations:</p>
<div class="table-wrapper"><table><thead><tr><th>Operation</th><th>Support Notes</th></tr></thead><tbody>
<tr><td>Pointwise arithmetic ops</td><td><code>+</code>, <code>-</code>, <code>*</code>, <code>/</code></td></tr>
<tr><td>Inequality</td><td><code>&gt;</code>, <code>&lt;</code>, <code>&gt;=</code>, <code>&lt;=</code></td></tr>
<tr><td><code>max</code> and <code>min</code></td><td></td></tr>
<tr><td>Clamp ops</td><td><code>clamp</code>, <code>relu</code></td></tr>
<tr><td>Matrix multiplication</td><td><code>Linear</code>; <code>matmul</code> / <code>@</code> where one side is a constant</td></tr>
<tr><td>Convolution</td><td>Via <code>vollo_torch.nn.PaddedConv1d</code></td></tr>
<tr><td>LSTM</td><td>Via <code>vollo_torch.nn.LSTM</code></td></tr>
<tr><td>Indexing / slicing</td><td>Partial square bracket <code>[]</code> support; <code>index_select</code></td></tr>
<tr><td><code>sum</code></td><td><code>keepdim = True</code> required when summing over data dim</td></tr>
<tr><td><code>where</code></td><td>If the <code>where</code> condition is an inequality comparison</td></tr>
<tr><td>Concatenation</td><td><code>cat</code>, <code>concat</code></td></tr>
<tr><td><code>LayerNorm</code></td><td></td></tr>
<tr><td><code>RMSNorm</code></td><td>via <code>vollo_torch.nn.RMSNorm</code> for torch versions &lt; 2.4</td></tr>
<tr><td>Batch Normalization</td><td><code>BatchNorm1d</code>, <code>BatchNorm2d</code>, <code>BatchNorm3d</code></td></tr>
<tr><td><code>transpose</code></td><td>See <a href="supported-models.html#tensor-memory-format">section below</a></td></tr>
<tr><td><code>squeeze</code>, <code>unsqueeze</code></td><td></td></tr>
<tr><td><code>sqrt</code></td><td><code>torch.sqrt</code>, <code>torch.rsqrt</code></td></tr>
</tbody></table>
</div>
<p>Note that for operations like <code>Dropout</code> and <code>BatchNorm1d</code> (which change behaviour at inference time) to be handled correctly, the model should be in <code>eval</code> mode.</p>
<h2 id="tensor-memory-format"><a class="header" href="#tensor-memory-format">Tensor Memory Format</a></h2>
<p>Vollo supports operations on tensors in <em>data-</em> or <em>channels-</em> last memory
format, i.e. the innermost dimension of the tensors should be the <em>data</em> or
<em>channels</em> dimension rather than the <em>batch</em> or <em>sequence</em> dimension if there is
one.
This is because the Vollo accelerator's compute units operate on contiguous
vectors (1D tensors) and has limited support for rearranging tensor data,
particularly transposing them.</p>
<p>There are some notable exceptions that <em>do not</em> require channels-last tensors:</p>
<ul>
<li>Layers that operate on sequences: <code>Conv1d</code>, <code>LSTM</code>.
Vollo supports the same (<em>batch</em>, <em>channels</em>, <em>sequence</em>) memory format that
PyTorch uses for these layers, but requires applying the <a href="example-2-cnn.html#using-the-streaming-transform">streaming
transform</a> to models that
contain them.</li>
<li>General matrix multiplication (as opposed to the more restrictive <code>Linear</code>):
<code>matmul</code>, <code>@</code>.</li>
</ul>
<h2 id="torchscript"><a class="header" href="#torchscript">TorchScript</a></h2>
<p>The Vollo compiler supports standard PyTorch modules (<code>torch.nn.Module</code>); it
does not support TorchScript modules (<code>torch.jit.ScriptModule</code>).</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="example-1-mlp"><a class="header" href="#example-1-mlp">Example 1: MLP</a></h1>
<p>Basic models like multilayer perceptrons (MLP) can be defined without any
changes from a standard PyTorch definition.</p>
<pre><code class="language-python">import torch
import torch.nn as nn
import torch.nn.functional as F

class MLP(nn.Module):
    def __init__(self, input_size, output_size, hidden_size):
        super().__init__()
        self.fc1 = nn.Linear(input_size, hidden_size)
        self.fc2 = nn.Linear(hidden_size, hidden_size)
        self.out = nn.Linear(hidden_size, output_size)

    def forward(self, x):
        x = F.relu(self.fc1(x))
        residual = x
        x = F.relu(self.fc2(x)) + residual
        return self.out(x)

# Instantiate the model
input_size = 784
output_size = 10
hidden_size = 128
model = MLP(input_size, output_size, hidden_size)
</code></pre>
<p>The first stage of compiling a model is to lower it to NNIR.
NNIR is the Vollo compiler's intermediate representation for representing neural
network graphs.
NNIR sits at a similar level of abstraction to ONNX, with most NNIR operators
having direct ONNX or PyTorch analogues.</p>
<pre><code class="language-python">import vollo_torch

# An input to the model needs to be provided so that its execution can be
# traced
input = torch.randn(input_size)
# Trace the model's execution to annotate it with activation shapes
(model, expected_output) = vollo_torch.fx.prepare_shape(model, input)
nnir = vollo_torch.fx.nnir.to_nnir(model)
</code></pre>
<p>NNIR can be compiled to a Vollo program given a Vollo accelerator configuration.</p>
<pre><code class="language-python">import vollo_compiler

config = vollo_compiler.Config.ia_420f_c6b32()
program = nnir.to_program(config)
</code></pre>
<p>Vollo programs have all their memory allocated statically.
You can print the static resource usage of a program like this:</p>
<pre><code class="language-python">print(program.metrics())
</code></pre>
<p>Save the program to a file so that it can be used for inference by the <a href="vollo-runtime.html">Vollo
runtime</a>.</p>
<pre><code class="language-python">program.save('mlp.vollo')
</code></pre>
<h2 id="simulation"><a class="header" href="#simulation">Simulation</a></h2>
<p>The Vollo compiler can be used to simulate programs in the Vollo virtual machine
(VM).
This is an instruction level simulation of the Vollo accelerator which can be
used to:</p>
<ul>
<li>Estimate performance of a model.
The VM is not cycle accurate but provides an indicative cycle count of a
model.</li>
<li>Verify the correctness of the compilation stages, including the effect of
quantisation.</li>
</ul>
<p>Construct a VM instance with your program loaded.
Run the VM by passing it a numpy array of the input.
It should produce the same result as the source PyTorch model, within some
range of floating point error.</p>
<pre><code class="language-python">vm = program.to_vm()
vm_output = vm.run(input.detach().numpy())
torch.testing.assert_close(expected_output, torch.from_numpy(vm_output), atol = 1e-2, rtol = 1e-2)
print("cycle count:", vm.cycle_count())
# Translate the estimated cycle count to a duration for the compute (not
# including IO) in microseconds, using the bitstream clock speed (320 MHz)
print(f"latency (compute): {vm.compute_duration_us():.1f}us")
</code></pre>
<p>The VM records the number of cycles the program took to execute.
Note there will be some discrepancy between the VM's cycle count and the true
cycle count, so the VM's cycle count should be treated as an estimate.
Also note that the VM does not model the latency of the communication between
the host and the Vollo accelerator. This communication latency can be estimated
using our <a href="benchmark-io.html"><code>IO Round Trip</code></a> benchmarks.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="example-2-cnn"><a class="header" href="#example-2-cnn">Example 2: CNN</a></h1>
<p>Vollo supports streaming 1D convolutional neural networks (CNNs), which might
require you to make some changes to your model if you are currently using a
non-streaming 1D CNN.</p>
<p>A streaming convolution applies the convolutional kernel to the most recent
window of the input sequence as the data points in the input sequence arrive.
This differs from a non-streaming convolution, which expects to receive a
complete input sequence and applies its convolutional kernel to each window of
that input.</p>
<p>Streaming convolutions will have much lower latency than non-streaming
convolutions, but they have to maintain some state, namely the most recent
window of input, making them unnatural to define in ML frameworks like PyTorch.
To enable the use of of streaming convolutions, the Vollo compiler includes a
<code>streaming_transform</code> which transforms a non-streaming CNN into a streaming CNN,
as long as the non-streaming CNN meets certain constraints.</p>
<h2 id="using-the-streaming_transform"><a class="header" href="#using-the-streaming_transform">Using the <code>streaming_transform</code></a></h2>
<p>The model below is a non-streaming CNN taking an input sequence of length 5 and
producing an output of length 1.
(It can actually take any input sequence of length 5+n and produce an output of
length 1+n, but we will only consider the minimal sequence length, since that is
the length of the input context used by each of the output elements.)</p>
<pre><code class="language-python">import torch
import torch.nn as nn
import torch.nn.functional as F

class CNN(nn.Module):
    def __init__(self, in_channels, out_channels, hidden_channels, kernel_size=3):
        super().__init__()
        # Reduces sequence length by (kernel_size - 1) = 2
        self.conv1 = nn.Conv1d(in_channels, hidden_channels, kernel_size)
        # Reduces sequence length by (kernel_size - 1) = 2
        self.conv2 = nn.Conv1d(hidden_channels, out_channels, kernel_size)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        x = F.relu(self.conv2(x))
        return x

# Instantiate the model
in_channels = 32
out_channels = 1
hidden_channels = 128
model = CNN(in_channels, out_channels, hidden_channels)
</code></pre>
<p>In order to apply the <code>streaming_transform</code>, the <code>torch.nn.Conv1d</code> layers need
to be replaced with <code>vollo_torch.nn.PaddedConv1d</code> layers.</p>
<pre><code class="language-python">import torch
import torch.nn as nn
import torch.nn.functional as F

import vollo_torch.nn

class CNN(nn.Module):
    def __init__(self, in_channels, out_channels, hidden_channels, kernel_size=3):
        super().__init__()
        self.conv1 = vollo_torch.nn.PaddedConv1d(in_channels, hidden_channels, kernel_size)
        self.conv2 = vollo_torch.nn.PaddedConv1d(hidden_channels, out_channels, kernel_size)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        x = F.relu(self.conv2(x))
        return x

# Instantiate the model
in_channels = 32
out_channels = 1
hidden_channels = 128
model = CNN(in_channels, out_channels, hidden_channels)
</code></pre>
<p>These <code>PaddedConv1d</code> layers are identical to <code>torch.nn.Conv1d</code>, but with left
padding pre-applied to the input so as not to reduce the sequence length.</p>
<p>This <code>PaddedConv1d</code> model is still a non-streaming model, which now takes an
input sequence of length 5 and produces an output of length 5.
Its relationship to the original <code>Conv1d</code> model is that, given the same model
parameters (weights, biases, etc.) and input sequence, the last element of the
output sequence of the <code>PaddedConv1d</code> model will be equal to the last/only
element of the output sequence of the <code>Conv1d</code> model.</p>
<p>The <code>PaddedConv1d</code> model can be lowered to NNIR and have the
<code>streaming_transform</code> applied.</p>
<pre><code class="language-python">batch_size = 1
sequence_length = 5
input = torch.randn(batch_size, in_channels, sequence_length)
(model, expected_output) = vollo_torch.fx.prepare_shape(model, input)
nnir = vollo_torch.fx.nnir.to_nnir(model)

# Provide the streaming transform with index of the sequence axis
(nnir, output_axis) = nnir.streaming_transform(2)
</code></pre>
<p>The resulting NNIR graph represents a streaming CNN, i.e. containing state, that
takes a single data point of a sequence as input and produces a single data
point as output, updating its input window state in the process.
Input sequences for the streaming CNN need to be fed in sequentially, e.g. in a
loop.
For example, using the VM:</p>
<pre><code class="language-python">import vollo_compiler

program = nnir.to_program(vollo_compiler.Config.ia_420f_c6b32())
vm = program.to_vm()

vm_outputs = []
for i in range(5):
    # Runs inference on one element of the input sequence, updating the
    # streaming CNN's state
    vm_outputs.append(vm.run(input[:, :, i].detach().numpy()))

torch.testing.assert_close(
    expected_output,
    torch.stack(
        [torch.from_numpy(output) for output in vm_outputs],
        axis=output_axis,
    ),
    atol = 1e-2,
    rtol = 1e-2
)
</code></pre>
<p>The streaming CNN satisfies the property that, given an input sequence, the i-th
element of the output sequence of the non-streaming CNN will be equal to the
output of the i-th iteration of feeding the input to the streaming CNN.</p>
<p>The streaming CNN can be saved and run on the accelerator like any other
program:</p>
<pre><code class="language-python">program.save('cnn.vollo')
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="example-3-multiple-models-in-a-vollo-program"><a class="header" href="#example-3-multiple-models-in-a-vollo-program">Example 3: Multiple Models in a Vollo Program</a></h1>
<p>Vollo supports putting multiple models on a single accelerator.</p>
<p>Multiple NNIRs can be compiled into a single program:</p>
<pre><code class="language-python"><span class="boring">import torch
</span><span class="boring">import torch.nn as nn
</span><span class="boring">import torch.nn.functional as F
</span><span class="boring">import vollo_torch
</span><span class="boring">
</span><span class="boring">class MLP(nn.Module):
</span><span class="boring">    def __init__(self, input_size, output_size, hidden_size):
</span><span class="boring">        super().__init__()
</span><span class="boring">        self.fc1 = nn.Linear(input_size, hidden_size)
</span><span class="boring">        self.fc2 = nn.Linear(hidden_size, hidden_size)
</span><span class="boring">        self.out = nn.Linear(hidden_size, output_size)
</span><span class="boring">
</span><span class="boring">    def forward(self, x):
</span><span class="boring">        x = F.relu(self.fc1(x))
</span><span class="boring">        residual = x
</span><span class="boring">        x = F.relu(self.fc2(x)) + residual
</span><span class="boring">        return self.out(x)
</span><span class="boring">
</span># Instantiate an MLP
input_size = 784
output_size = 10
hidden_size = 128
mlp_model = MLP(input_size, output_size, hidden_size)
mlp_input = torch.randn(input_size)
(mlp_model, mlp_expected_output) = vollo_torch.fx.prepare_shape(mlp_model, mlp_input)
mlp_nnir = vollo_torch.fx.nnir.to_nnir(mlp_model)

<span class="boring">class CNN(nn.Module):
</span><span class="boring">    def __init__(self, in_channels, out_channels, hidden_channels, kernel_size=3):
</span><span class="boring">        super().__init__()
</span><span class="boring">        self.conv1 = vollo_torch.nn.PaddedConv1d(in_channels, hidden_channels, kernel_size)
</span><span class="boring">        self.conv2 = vollo_torch.nn.PaddedConv1d(hidden_channels, out_channels, kernel_size)
</span><span class="boring">
</span><span class="boring">    def forward(self, x):
</span><span class="boring">        x = F.relu(self.conv1(x))
</span><span class="boring">        x = F.relu(self.conv2(x))
</span><span class="boring">        return x
</span><span class="boring">
</span># Instantiate a CNN
in_channels = 32
out_channels = 1
hidden_channels = 128
cnn_model = CNN(in_channels, out_channels, hidden_channels)

batch_size = 1
sequence_length = 5
cnn_input = torch.randn(batch_size, in_channels, sequence_length)
(cnn_model, cnn_expected_output) = vollo_torch.fx.prepare_shape(cnn_model, cnn_input)
cnn_nnir = vollo_torch.fx.nnir.to_nnir(cnn_model)
(cnn_nnir, output_axis) = cnn_nnir.streaming_transform(2)

# Compile the multi-model program
import vollo_compiler
program_builder = vollo_compiler.ProgramBuilder(vollo_compiler.Config.ia_420f_c6b32())
program_builder.add_nnir(mlp_nnir)
program_builder.add_nnir(cnn_nnir)
multi_model_program = program_builder.to_program()
</code></pre>
<p>The <code>vollo_compiler.ProgramBuilder</code> allows you to create a multi-model program. Building a multi-model program may give an allocation error if
the models can't fit on the given <code>Config</code>. Generally each individual model will only have a small latency overhead compared to running it as an individual program. This overhead comes from selecting which model to run.</p>
<p>A <code>model_index</code> can be provided when running inferences on the accelerator or on the VM. The models appear in the order in which they were added to the <code>ProgramBuilder</code>. For example on the VM:</p>
<pre><code class="language-python">vm = multi_model_program.to_vm()

mlp_vm_output = vm.run(mlp_input.detach().numpy(), model_index = 0)
torch.testing.assert_close(mlp_expected_output, torch.from_numpy(mlp_vm_output), atol = 1e-2, rtol = 1e-2)

cnn_vm_outputs = []
for i in range(5):
    cnn_vm_outputs.append(vm.run(cnn_input[:, :, i].detach().numpy(), model_index = 1))

torch.testing.assert_close(
    cnn_expected_output,
    torch.stack(
        [torch.from_numpy(output) for output in cnn_vm_outputs],
        axis=output_axis,
    ),
    atol = 1e-2,
    rtol = 1e-2
)
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="onnx-support"><a class="header" href="#onnx-support">ONNX Support</a></h1>
<p>Vollo also provides a tool for compiling ML models defined in ONNX.</p>
<p><code>vollo-onnx</code> is a command line tool which allows the user to specify an input ONNX file and produces a <code>.vollo</code> program as output. The user specifies a path to the input <code>.onnx</code> file:</p>
<pre><code class="language-text">Arguments:
  &lt;INPUT&gt;
          Path to the input .onnx file
</code></pre>
<p>The user can specify:</p>
<ul>
<li>
<p>The output path:</p>
<pre><code class="language-text">-o, --output &lt;OUTPUT&gt;
      Output path for the compiled program file

      [default: program.vollo]
</code></pre>
</li>
<li>
<p>A name for the model:</p>
<pre><code class="language-text">    --model-name &lt;MODEL_NAME&gt;
      Name of the model
</code></pre>
</li>
<li>
<p>The hardware configuration to use based on a JSON file (this JSON file can be generated using the <code>Config</code> method <code>save</code> in the <code>vollo_compiler</code> python module):</p>
<pre><code class="language-text">    --hw-config &lt;HW_CONFIG_JSON&gt;
      Path to the hardware config JSON file
</code></pre>
</li>
<li>
<p>A name for the hardware configuration to use (from a set of preset configs).</p>
<pre><code class="language-text">    --hw-config-preset &lt;PRESET_NAME&gt;
      Hardware configuration to use, chosen from a set of presets

      [possible values: ia420f-c6b32, ia840f-c3b64, ia840f-c2b64d]
</code></pre>
</li>
<li>
<p>Which transformations to perform on the model. Currently the only available transformation is the streaming transform <a href="example-2-cnn.html">Example 2: CNN</a>:</p>
<pre><code class="language-text">    --streaming-transform &lt;STREAMING_AXIS&gt;
      Axis on which to perform the streaming transform in the NNIR graph

      If unspecified, no streaming transform is performed
</code></pre>
</li>
<li>
<p>The input shape of the model. This is required if the ONNX model has dynamic input shapes. Vollo requires that the shape of the input be known at compile-time:</p>
<pre><code class="language-text">    --override-input-shape &lt;SHAPE&gt;
      If the model has dynamic input shapes, the user must pass a fixed input shape

      Example: 10,100,250
</code></pre>
</li>
<li>
<p>Whether to elide all compute logic and generate a program with only IO logic. This is useful for determining IO latencies.</p>
<pre><code class="language-text">    --io-only
      Generate a program with IO only - useful for testing IO latencies
</code></pre>
</li>
<li>
<p>Whether to disable certain optimizations in the compiler which increase compilation time.</p>
<pre><code class="language-text">    --disable-optimizations
        Disables some compiler optimizations. This can improve compilation time
</code></pre>
</li>
</ul>
<h2 id="simplifying-onnx-models"><a class="header" href="#simplifying-onnx-models">Simplifying ONNX Models</a></h2>
<p><code>vollo-onnx</code> has a limited list of supported ONNX nodes. Often ONNX models can be over-complicated, and contain unnecessary shaping operations. It is recommended that <a href="https://github.com/daquexian/onnx-simplifier">onnx-simplifier</a> be used before calling <code>vollo-onnx</code> on an ONNX model to remove these unnecessary shaping operations which aren't supported by <code>vollo-onnx</code>:</p>
<pre><code class="language-sh">onnx-sim &lt;model.onnx&gt; &lt;model-sim.onnx&gt; --overwrite-input-shape &lt;model-input-shape&gt;
</code></pre>
<p>It is also recommended to use the <code>--overwrite-input-shape</code> with <code>onnx-simplifier</code>, as this can enable further simplifications and better constant folding.</p>
<h2 id="using-onnx-from-python"><a class="header" href="#using-onnx-from-python">Using ONNX from Python</a></h2>
<p>ONNX models can also be imported and translated to NNIR models directly in python using the static <code>NNIR</code> method <code>from_onnx</code>. This also requires that the input shape be specified if the ONNX model has dynamic input shapes, otherwise it can be <code>None</code>.</p>
<pre><code class="language-python">onnx_nnir = vollo_compiler.NNIR.from_onnx(onnx_path, input_shape)
</code></pre>
<h2 id="supported-nodes"><a class="header" href="#supported-nodes">Supported Nodes</a></h2>
<p>Tensors are expected to be in <code>float32</code> format, unless they are used as indices / axes (in which case they should be <code>int64</code>s).</p>
<p><code>vollo-onnx</code> supports models with the following nodes:</p>
<div class="table-wrapper"><table><thead><tr><th>Operator</th><th>Support Notes</th></tr></thead><tbody>
<tr><td>Pointwise arithmetic ops</td><td><code>Add</code>, <code>Sub</code>, <code>Mul</code>, <code>Div</code></td></tr>
<tr><td>Inequality</td><td><code>&gt;</code>, <code>&lt;</code>, <code>&gt;=</code>, <code>&lt;=</code> (when followed by a <code>Where</code>)</td></tr>
<tr><td><code>Max</code> and <code>Min</code></td><td></td></tr>
<tr><td><code>Neg</code></td><td></td></tr>
<tr><td>Clamp ops</td><td><code>Clip</code>, <code>Relu</code></td></tr>
<tr><td>Matrix multiplication</td><td><code>MatMul</code> / <code>Gemm</code> where one input is a constant</td></tr>
<tr><td><code>Conv</code></td><td>1d with left-padding such that input and output seq dimensions match</td></tr>
<tr><td><code>LSTM</code></td><td>Forward LSTM without explicit hidden or cell state initialisation</td></tr>
<tr><td><code>Gather</code></td><td>With a 0d/1d tensor of indices</td></tr>
<tr><td><code>Slice</code></td><td><code>step</code> size 1 with constant <code>starts</code>, <code>ends</code> and <code>axes</code>.</td></tr>
<tr><td><code>ReduceSum</code></td><td>With constant axes, <code>keepdims = 1</code> required on data dimension</td></tr>
<tr><td><code>Where</code></td><td>If the <code>Where</code> condition is an inequality comparison</td></tr>
<tr><td><code>Concat</code></td><td>On outer dimension or at start or end of model</td></tr>
<tr><td><code>Transpose</code></td><td>See <a href="supported-models.html#tensor-memory-format">tensor memory format</a></td></tr>
<tr><td><code>LayerNormalization</code></td><td>With <code>axis = -1</code>. Supported in onnx opset versions &gt;= 17</td></tr>
<tr><td><code>BatchNormalization</code></td><td>Where input scale, bias, mean and var are constants</td></tr>
<tr><td><code>Squeeze</code>, <code>Unsqueeze</code></td><td></td></tr>
<tr><td><code>Reciprocal</code></td><td></td></tr>
<tr><td><code>Identity</code></td><td></td></tr>
<tr><td><code>Sqrt</code></td><td></td></tr>
</tbody></table>
</div><div style="break-before: page; page-break-before: always;"></div><h1 id="benchmarks"><a class="header" href="#benchmarks">Benchmarks</a></h1>
<p>This section provides benchmarks for the Vollo accelerator for a variety of models.</p>
<p>Performance figures are given for two configurations of the Vollo accelerator.
A three big core configuration which is provided for the IA-840F accelerator card
and a six small core configuration which is provided for the IA-420F accelerator card.
If you require a different configuration, please contact us at <a href="mailto:vollo@myrtle.ai">vollo@myrtle.ai</a>.</p>
<p>All these performance numbers can be measured using the <code>vollo-sdk</code> with the correct accelerator card
by running the provided <a href="running-the-benchmark.html">benchmark script</a>.</p>
<p>We also provide performance figures for a PCIe roundtrip for various input and output sizes.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="multilayer-perceptron-mlp"><a class="header" href="#multilayer-perceptron-mlp">Multilayer perceptron (MLP)</a></h1>
<p>The model below is a simple multilayer perceptron (MLP) with 3 layers.</p>
<pre><code class="language-python">class MLP(nn.Module):
    def __init__(self, input_size, hidden_size, output_size):
        super().__init__()
        self.fc1 = nn.Linear(input_size, hidden_size)
        self.fc2 = nn.Linear(hidden_size, hidden_size)
        self.fc3 = nn.Linear(hidden_size, output_size)

    def forward(self, x):
        x = nn.functional.relu(self.fc1(x))
        x = nn.functional.relu(self.fc2(x))
        return self.fc3(x)

mlp = MLP(256.0, 384.0, 128.0)
</code></pre>
<p>We demonstrate the model at a variety of batch sizes. The model has 295K parameters.</p>
<h2 id="ia-840f-3-big-cores"><a class="header" href="#ia-840f-3-big-cores">IA-840F: 3 big cores</a></h2>
<div class="table-wrapper"><table><thead><tr><th>Model</th><th>Batch size</th><th>Mean latency (μs)</th><th>99th Percentile latency (μs)</th></tr></thead><tbody>
<tr><td>mlp_b1</td><td>1</td><td>2.3</td><td>2.4</td></tr>
<tr><td>mlp_b4</td><td>4</td><td>2.5</td><td>2.6</td></tr>
<tr><td>mlp_b8</td><td>8</td><td>2.7</td><td>2.8</td></tr>
</tbody></table>
</div>
<h2 id="ia-420f-6-small-cores"><a class="header" href="#ia-420f-6-small-cores">IA-420F: 6 small cores</a></h2>
<div class="table-wrapper"><table><thead><tr><th>Model</th><th>Batch size</th><th>Mean latency (μs)</th><th>99th Percentile latency (μs)</th></tr></thead><tbody>
<tr><td>mlp_b1</td><td>1</td><td>2.9</td><td>3.0</td></tr>
<tr><td>mlp_b4</td><td>4</td><td>3.0</td><td>3.1</td></tr>
<tr><td>mlp_b8</td><td>8</td><td>3.4</td><td>3.5</td></tr>
</tbody></table>
</div><div style="break-before: page; page-break-before: always;"></div><h1 id="1d-convolutional-neural-networks-cnn"><a class="header" href="#1d-convolutional-neural-networks-cnn">1D Convolutional neural networks (CNN)</a></h1>
<p>We benchmark a simple 1-D convolutional model with a residual connection after every layer.</p>
<pre><code class="language-python">class ConvBlock(nn.Module):
    def __init__(self, channels, kernel_size):
        super().__init__()
        self.conv = vollo_torch.nn.PaddedConv1d(channels, channels, kernel_size)

    def forward(self, inp):
        x = self.conv(inp)
        return nn.functional.relu(x) + inp


class CNN(nn.Module):
    def __init__(self, num_layers, kernel_size, channels):
        super().__init__()
        assert num_layers &gt;= 1

        self.cnn = nn.Sequential(
            *[ConvBlock(channels, kernel_size) for i in range(num_layers)],
        )

    def forward(self, x):
        x = self.cnn(x)  # N x channels x T
        return x
</code></pre>
<h2 id="ia-840f-3-big-cores-1"><a class="header" href="#ia-840f-3-big-cores-1">IA-840F: 3 big cores</a></h2>
<div class="table-wrapper"><table><thead><tr><th>Model</th><th>Layers</th><th>Channels</th><th>Parameters</th><th>Mean latency (μs)</th><th>99th Percentile latency (μs)</th></tr></thead><tbody>
<tr><td>cnn_tiny</td><td>3</td><td>128</td><td>393K</td><td>2.2</td><td>2.2</td></tr>
<tr><td>cnn_small</td><td>3</td><td>256</td><td>1.6M</td><td>2.4</td><td>2.5</td></tr>
<tr><td>cnn_med</td><td>6</td><td>256</td><td>3.1M</td><td>3.0</td><td>3.2</td></tr>
</tbody></table>
</div>
<p>The kernel size for all models is 8.</p>
<h3 id="ia-420f-6-small-cores-1"><a class="header" href="#ia-420f-6-small-cores-1">IA-420F: 6 small cores</a></h3>
<div class="table-wrapper"><table><thead><tr><th>Model</th><th>Layers</th><th>Channels</th><th>Parameters</th><th>Mean latency (μs)</th><th>99th Percentile latency (μs)</th></tr></thead><tbody>
<tr><td>cnn_tiny</td><td>3</td><td>128</td><td>393K</td><td>2.2</td><td>2.3</td></tr>
<tr><td>cnn_small</td><td>3</td><td>256</td><td>1.6M</td><td>2.8</td><td>3.0</td></tr>
<tr><td>cnn_med</td><td>6</td><td>256</td><td>3.1M</td><td>3.9</td><td>3.9</td></tr>
</tbody></table>
</div>
<p>The kernel size for all models is 8.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="long-short-term-memory-lstm-networks"><a class="header" href="#long-short-term-memory-lstm-networks">Long Short Term Memory (LSTM) networks</a></h1>
<p>We benchmark an LSTM model consisting of a stack of LSTMs followed by a linear layer.</p>
<pre><code class="language-python">class LSTM(nn.Module):
    def __init__(self, num_layers, input_size, hidden_size, output_size):
        super().__init__()
        assert num_layers &gt;= 1
        self.lstm = vollo_torch.nn.LSTM(
            input_size, hidden_size, num_layers=num_layers
        )
        self.fc = nn.Linear(hidden_size, output_size)

    def forward(self, x):
        x = self.lstm(x)
        x = self.fc(x)
        return x
</code></pre>
<p>We have also had LSTM models benchmarked and audited as part of a STAC-ML submission where we hold the lowest latency across all models. Please refer to our STAC-ML submissions for more details:</p>
<ul>
<li>
<p><a href="https://www.stacresearch.com/MRTL221125">STAC ML Sumaco</a></p>
</li>
<li>
<p><a href="https://www.stacresearch.com/MRTL230426">STAC ML Tacana</a></p>
</li>
</ul>
<h2 id="ia-840f-3-big-cores-2"><a class="header" href="#ia-840f-3-big-cores-2">IA-840F: 3 big cores</a></h2>
<div class="table-wrapper"><table><thead><tr><th>Model</th><th>Layers</th><th>Hidden size</th><th>Parameters</th><th>Mean latency (μs)</th><th>99th Percentile latency (μs)</th></tr></thead><tbody>
<tr><td>lstm_tiny</td><td>2</td><td>128</td><td>266K</td><td>1.9</td><td>2.0</td></tr>
<tr><td>lstm_small</td><td>3</td><td>256</td><td>1.6M</td><td>3.0</td><td>3.1</td></tr>
<tr><td>lstm_med</td><td>3</td><td>480</td><td>5.5M</td><td>4.2</td><td>4.4</td></tr>
<tr><td>lstm_med_deep</td><td>6</td><td>320</td><td>4.9M</td><td>4.3</td><td>4.5</td></tr>
</tbody></table>
</div>
<p>The input size is the same as the hidden size for all models and the output size is set to 32. The layers refers to the number of
LSTM layers in the stack.</p>
<h2 id="ia-420f-6-small-cores-2"><a class="header" href="#ia-420f-6-small-cores-2">IA-420F: 6 small cores</a></h2>
<div class="table-wrapper"><table><thead><tr><th>Model</th><th>Layers</th><th>Hidden size</th><th>Parameters</th><th>Mean latency (μs)</th><th>99th Percentile latency (μs)</th></tr></thead><tbody>
<tr><td>lstm_tiny</td><td>2</td><td>128</td><td>266K</td><td>2.2</td><td>2.3</td></tr>
<tr><td>lstm_small</td><td>3</td><td>256</td><td>1.6M</td><td>4.2</td><td>4.4</td></tr>
</tbody></table>
</div>
<p>The input size is the same as the hidden size and the output size is set to 32.</p>
<p>The two medium models are not supported on the IA-420F accelerator card as they are too large to fit in the accelerator memory.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="io-round-trip"><a class="header" href="#io-round-trip">IO Round Trip</a></h1>
<p>The following IO round trip times are sampled by using a program with no compute on the
Vollo accelerator from the Vollo runtime.</p>
<p>More specifically this Vollo accelerator program waits for the last input byte to arrive
before it sends the first output byte back. This method takes into account some of the
overheads (such as copying to the DMA buffer in the Vollo runtime) associated with IO
and this test is set up to see how it scales with difference sizes of inputs and output
values.</p>
<p>The following tables shows the round trip times in <code>μs</code> on the <code>IA420F</code>
board (similar times were observed on <code>IA840F</code>), each value is a <code>bfloat16</code> (2 bytes),
using fewer than 32 values gives the same times as 32 values.</p>
<p>To reproduce these values on your own hardware run the provided <a href="running-the-benchmark.html">benchmark
script</a> with environment variable <code>RUN_IO_TEST=1</code>.</p>
<h2 id="user-buffers"><a class="header" href="#user-buffers">User buffers</a></h2>
<p>This includes copying data to/from DMA buffers.</p>
<p><strong>mean</strong>:</p>
<div class="table-wrapper"><table><thead><tr><th style="text-align: right">out\in</th><th style="text-align: right">32</th><th style="text-align: right">64</th><th style="text-align: right">128</th><th style="text-align: right">256</th><th style="text-align: right">512</th><th style="text-align: right">1024</th><th style="text-align: right">2048</th><th style="text-align: right">4096</th><th style="text-align: right">8192</th></tr></thead><tbody>
<tr><td style="text-align: right"><strong>32</strong></td><td style="text-align: right">1.8</td><td style="text-align: right">1.8</td><td style="text-align: right">1.9</td><td style="text-align: right">1.9</td><td style="text-align: right">1.9</td><td style="text-align: right">2.1</td><td style="text-align: right">2.7</td><td style="text-align: right">3.4</td><td style="text-align: right">4.6</td></tr>
<tr><td style="text-align: right"><strong>64</strong></td><td style="text-align: right">1.8</td><td style="text-align: right">1.9</td><td style="text-align: right">1.9</td><td style="text-align: right">1.9</td><td style="text-align: right">2.0</td><td style="text-align: right">2.2</td><td style="text-align: right">2.8</td><td style="text-align: right">3.3</td><td style="text-align: right">4.7</td></tr>
<tr><td style="text-align: right"><strong>128</strong></td><td style="text-align: right">1.9</td><td style="text-align: right">1.9</td><td style="text-align: right">1.9</td><td style="text-align: right">2.0</td><td style="text-align: right">2.0</td><td style="text-align: right">2.2</td><td style="text-align: right">2.7</td><td style="text-align: right">3.3</td><td style="text-align: right">4.7</td></tr>
<tr><td style="text-align: right"><strong>256</strong></td><td style="text-align: right">1.9</td><td style="text-align: right">1.9</td><td style="text-align: right">1.9</td><td style="text-align: right">2.0</td><td style="text-align: right">2.0</td><td style="text-align: right">2.2</td><td style="text-align: right">2.7</td><td style="text-align: right">3.4</td><td style="text-align: right">5.0</td></tr>
<tr><td style="text-align: right"><strong>512</strong></td><td style="text-align: right">1.9</td><td style="text-align: right">1.9</td><td style="text-align: right">2.0</td><td style="text-align: right">2.0</td><td style="text-align: right">2.1</td><td style="text-align: right">2.2</td><td style="text-align: right">2.8</td><td style="text-align: right">3.4</td><td style="text-align: right">4.8</td></tr>
<tr><td style="text-align: right"><strong>1024</strong></td><td style="text-align: right">2.2</td><td style="text-align: right">2.2</td><td style="text-align: right">2.2</td><td style="text-align: right">2.1</td><td style="text-align: right">2.3</td><td style="text-align: right">2.3</td><td style="text-align: right">2.7</td><td style="text-align: right">3.6</td><td style="text-align: right">4.9</td></tr>
<tr><td style="text-align: right"><strong>2048</strong></td><td style="text-align: right">2.4</td><td style="text-align: right">2.4</td><td style="text-align: right">2.5</td><td style="text-align: right">2.5</td><td style="text-align: right">2.5</td><td style="text-align: right">2.6</td><td style="text-align: right">2.8</td><td style="text-align: right">3.8</td><td style="text-align: right">5.0</td></tr>
<tr><td style="text-align: right"><strong>4096</strong></td><td style="text-align: right">2.9</td><td style="text-align: right">2.9</td><td style="text-align: right">2.9</td><td style="text-align: right">3.0</td><td style="text-align: right">3.0</td><td style="text-align: right">3.3</td><td style="text-align: right">3.6</td><td style="text-align: right">3.6</td><td style="text-align: right">5.2</td></tr>
<tr><td style="text-align: right"><strong>8192</strong></td><td style="text-align: right">3.8</td><td style="text-align: right">3.8</td><td style="text-align: right">3.9</td><td style="text-align: right">3.9</td><td style="text-align: right">3.9</td><td style="text-align: right">4.1</td><td style="text-align: right">4.7</td><td style="text-align: right">4.5</td><td style="text-align: right">5.1</td></tr>
</tbody></table>
</div>
<p><strong>p99</strong>:</p>
<div class="table-wrapper"><table><thead><tr><th style="text-align: right">out\in</th><th style="text-align: right">32</th><th style="text-align: right">64</th><th style="text-align: right">128</th><th style="text-align: right">256</th><th style="text-align: right">512</th><th style="text-align: right">1024</th><th style="text-align: right">2048</th><th style="text-align: right">4096</th><th style="text-align: right">8192</th></tr></thead><tbody>
<tr><td style="text-align: right"><strong>32</strong></td><td style="text-align: right">2.0</td><td style="text-align: right">2.0</td><td style="text-align: right">2.1</td><td style="text-align: right">2.0</td><td style="text-align: right">2.1</td><td style="text-align: right">2.4</td><td style="text-align: right">3.0</td><td style="text-align: right">3.7</td><td style="text-align: right">4.9</td></tr>
<tr><td style="text-align: right"><strong>64</strong></td><td style="text-align: right">2.0</td><td style="text-align: right">2.0</td><td style="text-align: right">2.1</td><td style="text-align: right">2.1</td><td style="text-align: right">2.1</td><td style="text-align: right">2.4</td><td style="text-align: right">3.0</td><td style="text-align: right">3.6</td><td style="text-align: right">5.0</td></tr>
<tr><td style="text-align: right"><strong>128</strong></td><td style="text-align: right">2.0</td><td style="text-align: right">2.0</td><td style="text-align: right">2.1</td><td style="text-align: right">2.1</td><td style="text-align: right">2.2</td><td style="text-align: right">2.4</td><td style="text-align: right">2.9</td><td style="text-align: right">3.6</td><td style="text-align: right">5.0</td></tr>
<tr><td style="text-align: right"><strong>256</strong></td><td style="text-align: right">2.0</td><td style="text-align: right">2.0</td><td style="text-align: right">2.0</td><td style="text-align: right">2.1</td><td style="text-align: right">2.2</td><td style="text-align: right">2.5</td><td style="text-align: right">3.1</td><td style="text-align: right">3.6</td><td style="text-align: right">5.3</td></tr>
<tr><td style="text-align: right"><strong>512</strong></td><td style="text-align: right">2.0</td><td style="text-align: right">2.1</td><td style="text-align: right">2.2</td><td style="text-align: right">2.1</td><td style="text-align: right">2.3</td><td style="text-align: right">2.4</td><td style="text-align: right">3.0</td><td style="text-align: right">3.7</td><td style="text-align: right">5.1</td></tr>
<tr><td style="text-align: right"><strong>1024</strong></td><td style="text-align: right">2.4</td><td style="text-align: right">2.4</td><td style="text-align: right">2.4</td><td style="text-align: right">2.4</td><td style="text-align: right">2.4</td><td style="text-align: right">2.5</td><td style="text-align: right">3.1</td><td style="text-align: right">3.8</td><td style="text-align: right">5.4</td></tr>
<tr><td style="text-align: right"><strong>2048</strong></td><td style="text-align: right">2.6</td><td style="text-align: right">2.6</td><td style="text-align: right">2.7</td><td style="text-align: right">2.7</td><td style="text-align: right">2.6</td><td style="text-align: right">2.8</td><td style="text-align: right">3.1</td><td style="text-align: right">4.2</td><td style="text-align: right">5.4</td></tr>
<tr><td style="text-align: right"><strong>4096</strong></td><td style="text-align: right">3.2</td><td style="text-align: right">3.2</td><td style="text-align: right">3.2</td><td style="text-align: right">3.3</td><td style="text-align: right">3.3</td><td style="text-align: right">3.6</td><td style="text-align: right">3.9</td><td style="text-align: right">3.8</td><td style="text-align: right">5.5</td></tr>
<tr><td style="text-align: right"><strong>8192</strong></td><td style="text-align: right">4.1</td><td style="text-align: right">4.1</td><td style="text-align: right">4.1</td><td style="text-align: right">4.1</td><td style="text-align: right">4.2</td><td style="text-align: right">4.4</td><td style="text-align: right">5.0</td><td style="text-align: right">4.7</td><td style="text-align: right">5.3</td></tr>
</tbody></table>
</div>
<h2 id="raw-dma-buffers"><a class="header" href="#raw-dma-buffers">Raw DMA buffers</a></h2>
<p>This is using buffers allocated with <code>vollo_rt_get_raw_buffer</code> which lets the runtime skip IO copy.</p>
<p><strong>mean</strong>:</p>
<div class="table-wrapper"><table><thead><tr><th style="text-align: right">out\in</th><th style="text-align: right">32</th><th style="text-align: right">64</th><th style="text-align: right">128</th><th style="text-align: right">256</th><th style="text-align: right">512</th><th style="text-align: right">1024</th><th style="text-align: right">2048</th><th style="text-align: right">4096</th><th style="text-align: right">8192</th></tr></thead><tbody>
<tr><td style="text-align: right"><strong>32</strong></td><td style="text-align: right">1.7</td><td style="text-align: right">1.8</td><td style="text-align: right">1.8</td><td style="text-align: right">1.8</td><td style="text-align: right">1.9</td><td style="text-align: right">1.9</td><td style="text-align: right">2.0</td><td style="text-align: right">2.2</td><td style="text-align: right">2.6</td></tr>
<tr><td style="text-align: right"><strong>64</strong></td><td style="text-align: right">1.7</td><td style="text-align: right">1.8</td><td style="text-align: right">1.8</td><td style="text-align: right">1.9</td><td style="text-align: right">1.9</td><td style="text-align: right">1.9</td><td style="text-align: right">2.0</td><td style="text-align: right">2.2</td><td style="text-align: right">2.7</td></tr>
<tr><td style="text-align: right"><strong>128</strong></td><td style="text-align: right">1.8</td><td style="text-align: right">1.8</td><td style="text-align: right">1.8</td><td style="text-align: right">1.8</td><td style="text-align: right">1.9</td><td style="text-align: right">1.9</td><td style="text-align: right">2.0</td><td style="text-align: right">2.2</td><td style="text-align: right">2.6</td></tr>
<tr><td style="text-align: right"><strong>256</strong></td><td style="text-align: right">1.8</td><td style="text-align: right">1.8</td><td style="text-align: right">1.8</td><td style="text-align: right">1.9</td><td style="text-align: right">1.8</td><td style="text-align: right">1.9</td><td style="text-align: right">2.1</td><td style="text-align: right">2.2</td><td style="text-align: right">2.6</td></tr>
<tr><td style="text-align: right"><strong>512</strong></td><td style="text-align: right">1.8</td><td style="text-align: right">1.8</td><td style="text-align: right">1.8</td><td style="text-align: right">1.8</td><td style="text-align: right">1.9</td><td style="text-align: right">1.9</td><td style="text-align: right">2.1</td><td style="text-align: right">2.3</td><td style="text-align: right">2.6</td></tr>
<tr><td style="text-align: right"><strong>1024</strong></td><td style="text-align: right">1.9</td><td style="text-align: right">1.9</td><td style="text-align: right">1.9</td><td style="text-align: right">1.9</td><td style="text-align: right">1.9</td><td style="text-align: right">2.0</td><td style="text-align: right">2.1</td><td style="text-align: right">2.3</td><td style="text-align: right">2.7</td></tr>
<tr><td style="text-align: right"><strong>2048</strong></td><td style="text-align: right">1.9</td><td style="text-align: right">1.9</td><td style="text-align: right">2.0</td><td style="text-align: right">2.0</td><td style="text-align: right">2.0</td><td style="text-align: right">2.1</td><td style="text-align: right">2.2</td><td style="text-align: right">2.4</td><td style="text-align: right">2.8</td></tr>
<tr><td style="text-align: right"><strong>4096</strong></td><td style="text-align: right">2.2</td><td style="text-align: right">2.2</td><td style="text-align: right">2.2</td><td style="text-align: right">2.2</td><td style="text-align: right">2.2</td><td style="text-align: right">2.3</td><td style="text-align: right">2.4</td><td style="text-align: right">2.6</td><td style="text-align: right">3.0</td></tr>
<tr><td style="text-align: right"><strong>8192</strong></td><td style="text-align: right">2.5</td><td style="text-align: right">2.5</td><td style="text-align: right">2.6</td><td style="text-align: right">2.6</td><td style="text-align: right">2.6</td><td style="text-align: right">2.7</td><td style="text-align: right">2.8</td><td style="text-align: right">3.0</td><td style="text-align: right">3.4</td></tr>
</tbody></table>
</div>
<p><strong>p99</strong>:</p>
<div class="table-wrapper"><table><thead><tr><th style="text-align: right">out\in</th><th style="text-align: right">32</th><th style="text-align: right">64</th><th style="text-align: right">128</th><th style="text-align: right">256</th><th style="text-align: right">512</th><th style="text-align: right">1024</th><th style="text-align: right">2048</th><th style="text-align: right">4096</th><th style="text-align: right">8192</th></tr></thead><tbody>
<tr><td style="text-align: right"><strong>32</strong></td><td style="text-align: right">1.9</td><td style="text-align: right">1.9</td><td style="text-align: right">1.9</td><td style="text-align: right">1.9</td><td style="text-align: right">2.1</td><td style="text-align: right">2.1</td><td style="text-align: right">2.2</td><td style="text-align: right">2.4</td><td style="text-align: right">2.9</td></tr>
<tr><td style="text-align: right"><strong>64</strong></td><td style="text-align: right">1.9</td><td style="text-align: right">1.9</td><td style="text-align: right">2.0</td><td style="text-align: right">1.9</td><td style="text-align: right">2.0</td><td style="text-align: right">2.1</td><td style="text-align: right">2.2</td><td style="text-align: right">2.4</td><td style="text-align: right">2.9</td></tr>
<tr><td style="text-align: right"><strong>128</strong></td><td style="text-align: right">1.9</td><td style="text-align: right">1.9</td><td style="text-align: right">1.9</td><td style="text-align: right">1.9</td><td style="text-align: right">2.1</td><td style="text-align: right">2.0</td><td style="text-align: right">2.2</td><td style="text-align: right">2.4</td><td style="text-align: right">2.9</td></tr>
<tr><td style="text-align: right"><strong>256</strong></td><td style="text-align: right">1.9</td><td style="text-align: right">2.0</td><td style="text-align: right">2.0</td><td style="text-align: right">1.9</td><td style="text-align: right">2.0</td><td style="text-align: right">2.1</td><td style="text-align: right">2.2</td><td style="text-align: right">2.4</td><td style="text-align: right">2.8</td></tr>
<tr><td style="text-align: right"><strong>512</strong></td><td style="text-align: right">1.9</td><td style="text-align: right">2.0</td><td style="text-align: right">2.0</td><td style="text-align: right">2.0</td><td style="text-align: right">2.0</td><td style="text-align: right">2.1</td><td style="text-align: right">2.2</td><td style="text-align: right">2.5</td><td style="text-align: right">2.9</td></tr>
<tr><td style="text-align: right"><strong>1024</strong></td><td style="text-align: right">2.1</td><td style="text-align: right">2.0</td><td style="text-align: right">2.0</td><td style="text-align: right">2.1</td><td style="text-align: right">2.1</td><td style="text-align: right">2.1</td><td style="text-align: right">2.3</td><td style="text-align: right">2.6</td><td style="text-align: right">2.9</td></tr>
<tr><td style="text-align: right"><strong>2048</strong></td><td style="text-align: right">2.1</td><td style="text-align: right">2.1</td><td style="text-align: right">2.1</td><td style="text-align: right">2.2</td><td style="text-align: right">2.2</td><td style="text-align: right">2.2</td><td style="text-align: right">2.3</td><td style="text-align: right">2.6</td><td style="text-align: right">3.0</td></tr>
<tr><td style="text-align: right"><strong>4096</strong></td><td style="text-align: right">2.3</td><td style="text-align: right">2.3</td><td style="text-align: right">2.3</td><td style="text-align: right">2.4</td><td style="text-align: right">2.4</td><td style="text-align: right">2.5</td><td style="text-align: right">2.6</td><td style="text-align: right">2.9</td><td style="text-align: right">3.4</td></tr>
<tr><td style="text-align: right"><strong>8192</strong></td><td style="text-align: right">2.8</td><td style="text-align: right">2.7</td><td style="text-align: right">2.7</td><td style="text-align: right">2.7</td><td style="text-align: right">2.8</td><td style="text-align: right">2.8</td><td style="text-align: right">3.0</td><td style="text-align: right">3.2</td><td style="text-align: right">3.6</td></tr>
</tbody></table>
</div>
<h2 id="mmio"><a class="header" href="#mmio">MMIO</a></h2>
<p>This is skipping DMA for the input (raw DMA buffers are used for the output).
It is configured via the <code>VOLLO_MMIO_MAX_SIZE</code> environment variable.
Typically MMIO would only be used for smaller inputs, but this table shows it used for the entire input.</p>
<p><strong>mean</strong>:</p>
<div class="table-wrapper"><table><thead><tr><th style="text-align: right">out\in</th><th style="text-align: right">32</th><th style="text-align: right">64</th><th style="text-align: right">128</th><th style="text-align: right">256</th><th style="text-align: right">512</th><th style="text-align: right">1024</th><th style="text-align: right">2048</th><th style="text-align: right">4096</th><th style="text-align: right">8192</th></tr></thead><tbody>
<tr><td style="text-align: right"><strong>32</strong></td><td style="text-align: right">1.0</td><td style="text-align: right">1.3</td><td style="text-align: right">1.5</td><td style="text-align: right">2.0</td><td style="text-align: right">3.1</td><td style="text-align: right">5.4</td><td style="text-align: right">9.6</td><td style="text-align: right">18.5</td><td style="text-align: right">35.9</td></tr>
<tr><td style="text-align: right"><strong>64</strong></td><td style="text-align: right">1.1</td><td style="text-align: right">1.2</td><td style="text-align: right">1.5</td><td style="text-align: right">2.0</td><td style="text-align: right">3.1</td><td style="text-align: right">5.3</td><td style="text-align: right">9.7</td><td style="text-align: right">18.4</td><td style="text-align: right">34.8</td></tr>
<tr><td style="text-align: right"><strong>128</strong></td><td style="text-align: right">1.1</td><td style="text-align: right">1.2</td><td style="text-align: right">1.5</td><td style="text-align: right">2.0</td><td style="text-align: right">3.2</td><td style="text-align: right">5.2</td><td style="text-align: right">9.5</td><td style="text-align: right">18.4</td><td style="text-align: right">35.8</td></tr>
<tr><td style="text-align: right"><strong>256</strong></td><td style="text-align: right">1.1</td><td style="text-align: right">1.2</td><td style="text-align: right">1.5</td><td style="text-align: right">2.0</td><td style="text-align: right">3.1</td><td style="text-align: right">5.2</td><td style="text-align: right">9.7</td><td style="text-align: right">18.4</td><td style="text-align: right">35.9</td></tr>
<tr><td style="text-align: right"><strong>512</strong></td><td style="text-align: right">1.1</td><td style="text-align: right">1.2</td><td style="text-align: right">1.5</td><td style="text-align: right">2.1</td><td style="text-align: right">3.1</td><td style="text-align: right">5.3</td><td style="text-align: right">9.7</td><td style="text-align: right">18.4</td><td style="text-align: right">35.8</td></tr>
<tr><td style="text-align: right"><strong>1024</strong></td><td style="text-align: right">1.2</td><td style="text-align: right">1.3</td><td style="text-align: right">1.6</td><td style="text-align: right">2.1</td><td style="text-align: right">3.2</td><td style="text-align: right">5.3</td><td style="text-align: right">9.8</td><td style="text-align: right">18.5</td><td style="text-align: right">36.0</td></tr>
<tr><td style="text-align: right"><strong>2048</strong></td><td style="text-align: right">1.2</td><td style="text-align: right">1.4</td><td style="text-align: right">1.7</td><td style="text-align: right">2.2</td><td style="text-align: right">3.2</td><td style="text-align: right">5.5</td><td style="text-align: right">9.9</td><td style="text-align: right">18.5</td><td style="text-align: right">35.9</td></tr>
<tr><td style="text-align: right"><strong>4096</strong></td><td style="text-align: right">1.5</td><td style="text-align: right">1.6</td><td style="text-align: right">1.9</td><td style="text-align: right">2.4</td><td style="text-align: right">3.6</td><td style="text-align: right">5.7</td><td style="text-align: right">10.1</td><td style="text-align: right">18.8</td><td style="text-align: right">36.1</td></tr>
<tr><td style="text-align: right"><strong>8192</strong></td><td style="text-align: right">1.9</td><td style="text-align: right">2.0</td><td style="text-align: right">2.2</td><td style="text-align: right">2.8</td><td style="text-align: right">3.9</td><td style="text-align: right">6.1</td><td style="text-align: right">10.4</td><td style="text-align: right">19.2</td><td style="text-align: right">36.7</td></tr>
</tbody></table>
</div>
<p><strong>p99</strong>:</p>
<div class="table-wrapper"><table><thead><tr><th style="text-align: right">out\in</th><th style="text-align: right">32</th><th style="text-align: right">64</th><th style="text-align: right">128</th><th style="text-align: right">256</th><th style="text-align: right">512</th><th style="text-align: right">1024</th><th style="text-align: right">2048</th><th style="text-align: right">4096</th><th style="text-align: right">8192</th></tr></thead><tbody>
<tr><td style="text-align: right"><strong>32</strong></td><td style="text-align: right">1.2</td><td style="text-align: right">1.5</td><td style="text-align: right">1.6</td><td style="text-align: right">2.1</td><td style="text-align: right">3.2</td><td style="text-align: right">5.5</td><td style="text-align: right">9.9</td><td style="text-align: right">19.1</td><td style="text-align: right">37.0</td></tr>
<tr><td style="text-align: right"><strong>64</strong></td><td style="text-align: right">1.2</td><td style="text-align: right">1.4</td><td style="text-align: right">1.6</td><td style="text-align: right">2.2</td><td style="text-align: right">3.2</td><td style="text-align: right">5.5</td><td style="text-align: right">9.8</td><td style="text-align: right">19.0</td><td style="text-align: right">35.9</td></tr>
<tr><td style="text-align: right"><strong>128</strong></td><td style="text-align: right">1.2</td><td style="text-align: right">1.3</td><td style="text-align: right">1.6</td><td style="text-align: right">2.1</td><td style="text-align: right">3.4</td><td style="text-align: right">5.5</td><td style="text-align: right">10.0</td><td style="text-align: right">19.1</td><td style="text-align: right">36.9</td></tr>
<tr><td style="text-align: right"><strong>256</strong></td><td style="text-align: right">1.2</td><td style="text-align: right">1.4</td><td style="text-align: right">1.6</td><td style="text-align: right">2.2</td><td style="text-align: right">3.2</td><td style="text-align: right">5.3</td><td style="text-align: right">9.8</td><td style="text-align: right">19.1</td><td style="text-align: right">36.8</td></tr>
<tr><td style="text-align: right"><strong>512</strong></td><td style="text-align: right">1.3</td><td style="text-align: right">1.4</td><td style="text-align: right">1.6</td><td style="text-align: right">2.2</td><td style="text-align: right">3.3</td><td style="text-align: right">5.5</td><td style="text-align: right">9.9</td><td style="text-align: right">19.1</td><td style="text-align: right">36.8</td></tr>
<tr><td style="text-align: right"><strong>1024</strong></td><td style="text-align: right">1.5</td><td style="text-align: right">1.4</td><td style="text-align: right">1.7</td><td style="text-align: right">2.2</td><td style="text-align: right">3.3</td><td style="text-align: right">5.5</td><td style="text-align: right">9.8</td><td style="text-align: right">19.2</td><td style="text-align: right">37.0</td></tr>
<tr><td style="text-align: right"><strong>2048</strong></td><td style="text-align: right">1.3</td><td style="text-align: right">1.4</td><td style="text-align: right">1.8</td><td style="text-align: right">2.4</td><td style="text-align: right">3.4</td><td style="text-align: right">5.5</td><td style="text-align: right">10.1</td><td style="text-align: right">19.2</td><td style="text-align: right">36.9</td></tr>
<tr><td style="text-align: right"><strong>4096</strong></td><td style="text-align: right">1.6</td><td style="text-align: right">1.8</td><td style="text-align: right">2.0</td><td style="text-align: right">2.5</td><td style="text-align: right">3.6</td><td style="text-align: right">5.9</td><td style="text-align: right">10.2</td><td style="text-align: right">19.4</td><td style="text-align: right">37.1</td></tr>
<tr><td style="text-align: right"><strong>8192</strong></td><td style="text-align: right">2.0</td><td style="text-align: right">2.1</td><td style="text-align: right">2.3</td><td style="text-align: right">2.9</td><td style="text-align: right">4.0</td><td style="text-align: right">6.3</td><td style="text-align: right">10.6</td><td style="text-align: right">19.8</td><td style="text-align: right">37.6</td></tr>
</tbody></table>
</div><div style="break-before: page; page-break-before: always;"></div><h1 id="setting-up-the-vollo-accelerator"><a class="header" href="#setting-up-the-vollo-accelerator">Setting up the Vollo accelerator</a></h1>
<p>This section describes how to program your accelerator card with the Vollo
Accelerator upon first use and how to reprogram your accelerator card with
updated versions of the Vollo Accelerator.
It also describes how to obtain a Vollo license which you will need to use the
Vollo accelerator.</p>
<h2 id="environment-variable-setup"><a class="header" href="#environment-variable-setup">Environment Variable Setup</a></h2>
<p>The initial setup instructions should be run in the Vollo SDK directory.</p>
<pre><code class="language-bash">cd vollo-sdk-&lt;VERSION&gt;
</code></pre>
<p>When using Vollo, you should also have the <code>setup.sh</code> script sourced in <code>bash</code>
to set up environment variables used by Vollo:</p>
<pre><code class="language-bash">source setup.sh
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="system-requirements"><a class="header" href="#system-requirements">System Requirements</a></h1>
<h2 id="cpu-requirements"><a class="header" href="#cpu-requirements">CPU Requirements</a></h2>
<p>The minimum CPU specification for the system is shown below.</p>
<ul>
<li>Single Socket 6 core Intel Xeon CPU at 2.0 GHz, equivalent AMD processor or better.</li>
<li>8 GB RAM</li>
</ul>
<h2 id="accelerator-card-requirements"><a class="header" href="#accelerator-card-requirements">Accelerator Card Requirements</a></h2>
<p>The SDK runs on a server CPU with PCIe FPGA accelerator cards.
It currently supports the following accelerator cards:</p>
<div class="table-wrapper"><table><thead><tr><th>Accelerator Card</th><th>FPGA</th><th>Max parameter count</th></tr></thead><tbody>
<tr><td>BittWare IA-420f</td><td>Intel Agilex AGF014</td><td>3 Million</td></tr>
<tr><td>BittWare IA-840f</td><td>Intel Agilex AGF027</td><td>8 Million</td></tr>
</tbody></table>
</div>
<h2 id="operating-system-requirements"><a class="header" href="#operating-system-requirements">Operating System Requirements</a></h2>
<p>Vollo is compatible with Ubuntu 20.04 and later.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="programming-the-fpga"><a class="header" href="#programming-the-fpga">Programming the FPGA</a></h1>
<h2 id="download-the-bitstream-for-your-fpga"><a class="header" href="#download-the-bitstream-for-your-fpga">Download the bitstream for your FPGA</a></h2>
<p>The bitstream is available on the Github Release page alongside the Vollo SDK.
For example to download the bitstream for the Agilex <code>ia840f</code> board with the <code>c2b64d</code> configuration of Vollo:</p>
<pre><code class="language-sh">wget https://github.com/MyrtleSoftware/vollo-sdk/releases/download/v20.0.3/vollo-ia840f-c2b64d-20.0.tar.gz
mkdir -p $VOLLO_SDK/bitstream
tar -xzf vollo-ia840f-c2b64d-20.0.tar.gz -C $VOLLO_SDK/bitstream
</code></pre>
<h2 id="programming-the-fpga-via-jtag"><a class="header" href="#programming-the-fpga-via-jtag">Programming the FPGA via JTAG</a></h2>
<p>If your FPGA is not already programmed with the Vollo accelerator then please
follow these instructions to load the bitstream into the accelerator card's
flash memory.</p>
<p>This requires a USB cable to be connected to the accelerator card and Quartus
programmer to be installed on the system so that the device can be programmed
over JTAG.</p>
<p>If the FPGA card already has a Vollo Accelerator Bitstream, it can be updated
over PCIe by following the steps in the section <a href="programming-the-fpga.html#programming-the-fpga-via-pcie">Program the FPGA via
PCIe</a> below.
Note that you only need to update the bitstream if updating to an <a href="versions.html#version-compatibility">incompatible
version</a> of the Vollo SDK.
Programming over PCIe is faster than programming over JTAG, and does not
require a USB programming cable or for Quartus Programmer to be installed.</p>
<ol>
<li>
<p>Download and install the latest Quartus Programmer:</p>
<ul>
<li>Navigate to
<a href="https://www.intel.com/content/www/us/en/software-kit/782411/intel-quartus-prime-pro-edition-design-software-version-23-2-for-linux.html">https://www.intel.com/content/www/us/en/software-kit/782411/intel-quartus-prime-pro-edition-design-software-version-23-2-for-linux.html</a>.</li>
<li>Select <code>Additional Software</code> and scroll down to find the Programmer.</li>
<li>Follow the instructions for installation.</li>
</ul>
</li>
<li>
<p>Add Quartus programmer to your path:</p>
<pre><code class="language-sh">export QUARTUS_DIR=&lt;path to qprogrammer install&gt;
export PATH=$QUARTUS_DIR/qprogrammer/quartus/bin:$PATH
</code></pre>
</li>
<li>
<p>Start the jtag daemon:</p>
<pre><code class="language-sh">sudo killall jtagd
sudo jtagd
</code></pre>
</li>
<li>
<p>Run <code>jtagconfig</code> from the Quartus install, you should see the device(s):</p>
<pre><code class="language-sh">$ jtagconfig
1) IA-840F [1-5.2]
  0341B0DD   AGFB027R25A(.|R0)
</code></pre>
</li>
<li>
<p>Navigate to the directory containing the <code>jic</code> file:</p>
<pre><code class="language-sh">source setup.sh
cd $VOLLO_SDK/bitstream
</code></pre>
</li>
<li>
<p>Set the JTAG clock frequency of the device you want to program to 16 MHz.
Specify the device by providing the name returned by <code>jtagconfig</code>:</p>
<pre><code class="language-sh">jtagconfig --setparam "IA-840F [1-5.2]" JtagClock 16M
</code></pre>
</li>
<li>
<p>Start the programming operation on the chosen device. This takes around 20
minutes. For the IA840F:</p>
<pre><code class="language-sh">quartus_pgm -c "IA-840F [1-5.2]" -m JTAG -o "ipv;vollo-ia840f-c3b64.jic"
</code></pre>
<p>Or for IA420F:</p>
<pre><code class="language-sh">quartus_pgm -c "IA-420F [1-5.2]" -m JTAG -o "ipv;vollo-ia420f-c6b32.jic"
</code></pre>
</li>
<li>
<p>Go back to 6 and program any other devices.</p>
</li>
<li>
<p>Power off the system and start it back up. The bitstream will now be loaded
onto the FPGA.</p>
<blockquote>
<p>⚠️ For the configuration process to be triggered the board has to register
the power being off. It is recommended to turn the power off and then wait
a few seconds before turning the power back on to ensure this happens.</p>
</blockquote>
</li>
<li>
<p>Check a Vollo bitstream is loaded:</p>
<pre><code class="language-sh">$ lspci -d 1ed9:766f
51:00.0 Processing accelerators: Myrtle.ai Device 766f (rev 01)
</code></pre>
<p>Check the correct Vollo bitstream is loaded:</p>
<pre><code class="language-sh">vollo-tool bitstream-check bitstream/&lt;bitstream-name&gt;.json
</code></pre>
</li>
</ol>
<h2 id="programming-the-fpga-via-pcie"><a class="header" href="#programming-the-fpga-via-pcie">Programming the FPGA via PCIe</a></h2>
<p>NOTE: this can only be done with an FPGA that is already programmed with a Vollo bitstream.</p>
<ol>
<li>
<p>Load the kernel driver:</p>
<pre><code class="language-sh">sudo ./load-kernel-driver.sh
</code></pre>
</li>
<li>
<p>Check the current bitstream information:</p>
<pre><code class="language-sh">source setup.sh
vollo-tool bitstream-info
</code></pre>
</li>
<li>
<p>Check that the device is set up for remote system updates by running the
command below, with <code>device index</code> representing the index of the device you
want to update, in the order shown in the previous command, starting from 0.
It should print a <code>json</code> string to the terminal showing the device status.</p>
<pre><code class="language-sh">vollo-tool fpga-config rsu-status &lt;device index&gt;
</code></pre>
</li>
<li>
<p>Update the <code>USER_IMAGE</code> partition of the flash with the new bitstream image
contained in the <code>rpd</code> archive in the <code>$VOLLO_SDK/bitstream</code> directory. This should take
around 5 minutes. Do not interrupt this process until it completes.</p>
<pre><code class="language-sh">sudo ./load-kernel-driver.sh
vollo-tool fpga-config overwrite-partition &lt;device index&gt; &lt;.rpd.tar.gz file&gt; USER_IMAGE
</code></pre>
</li>
<li>
<p>Repeat step 4 for any other devices you wish to update.</p>
</li>
<li>
<p>Power off the system and start it back up.</p>
<blockquote>
<p>⚠️ For the configuration process to be triggered the board has to register
the power being off. It is recommended to turn the power off and then wait
a few seconds before turning the power back on to ensure this happens.</p>
</blockquote>
</li>
<li>
<p>Repeat steps 1, 2 and 3. The <code>bitstream-info</code> command should show that the
updated bitstream has been loaded (e.g. a newer release date), and the output
of the <code>rsu-status</code> command should show all zeroes for the <code>error_code</code> and
<code>failing_image_address</code> fields.</p>
</li>
<li>
<p>Check the correct Vollo bitstream is loaded:</p>
<pre><code class="language-sh">sudo ./load-kernel-driver.sh
vollo-tool bitstream-check bitstream/&lt;bitstream-name&gt;.json
</code></pre>
</li>
</ol>
<div style="break-before: page; page-break-before: always;"></div><h1 id="licensing"><a class="header" href="#licensing">Licensing</a></h1>
<p>Vollo is licensed on a per-device basis.</p>
<h2 id="redeeming-licenses-with-vollo-tool"><a class="header" href="#redeeming-licenses-with-vollo-tool">Redeeming licenses with vollo-tool</a></h2>
<p>You will receive a <code>purchase-token</code> with your Vollo purchase. The <code>purchase-token</code> can be used to redeem Vollo licenses for a set number of devices.</p>
<p>To see the number of credits (i.e. the number of devices which can be redeemed) on your <code>purchase-token</code>, run:</p>
<pre><code class="language-sh">source setup.sh
vollo-tool license num-remaining-devices -t &lt;purchase-token&gt;
</code></pre>
<p>To redeem devices on your purchase token:</p>
<ol>
<li>
<p>Load the kernel driver if you haven't already done so:</p>
<pre><code class="language-sh">sudo ./load-kernel-driver.sh
</code></pre>
</li>
<li>
<p>Run <code>vollo-tool device-ids</code>. This will enumerate all Vollo accelerators and output their device IDs.</p>
<pre><code class="language-sh">vollo-tool device-ids | tee vollo.devices
</code></pre>
</li>
<li>
<p>Run <code>vollo-tool license redeem-device</code>, passing the device IDs you wish to generate licenses for. This will print a breakdown of which devices will consume credits on the <code>purchase-token</code>.</p>
<pre><code class="language-sh">vollo-tool license redeem-device -t &lt;purchase-token&gt; --device-ids &lt;device IDs&gt;
</code></pre>
<p>Alternatively you can pass the <code>vollo.devices</code> output from the previous step if you wish to redeem licenses for all devices.</p>
<pre><code class="language-sh">vollo-tool license redeem-device -t &lt;purchase-token&gt; --device-id-file &lt;device ID file&gt;
</code></pre>
</li>
<li>
<p>When you have confirmed which devices will consume credits on the <code>purchase-token</code>, run <code>vollo-tool license redeem-device --consume-credits</code> to generate the licenses.
The licenses will be printed to <code>stdout</code>.</p>
<pre><code class="language-sh">vollo-tool license redeem-device -t &lt;purchase-token&gt; --device-ids &lt;device IDs&gt; --consume-credits | tee vollo.lic
</code></pre>
</li>
</ol>
<p>The licenses redeemed on a purchase token can be viewed at any time by running <code>vollo-tool license view-licenses</code>:</p>
<pre><code class="language-sh">vollo-tool license view-licenses -t &lt;purchase-token&gt; | tee vollo.lic
</code></pre>
<h2 id="installing-a-license"><a class="header" href="#installing-a-license">Installing a license</a></h2>
<ol>
<li>
<p>The license file location should be set in the environment variable <code>MYRTLE_LICENSE</code>.</p>
<pre><code class="language-sh">export MYRTLE_LICENSE=&lt;license file&gt;
</code></pre>
</li>
<li>
<p>Check that the license for your device(s) is being recognised.</p>
<pre><code class="language-sh">vollo-tool license-check
</code></pre>
<p>If successful, the output should look like this:</p>
<pre><code class="language-output">Ok: found 2 devices with valid licenses
</code></pre>
</li>
</ol>
<div style="break-before: page; page-break-before: always;"></div><h1 id="running-an-example"><a class="header" href="#running-an-example">Running an example</a></h1>
<p>The Vollo SDK contains a trivial program for each accelerator to check if the accelerator is working.</p>
<ol>
<li>
<p>Ensure you have run the setup steps:</p>
<pre><code class="language-sh">cd &lt;vollo-sdk&gt;
sudo ./load-kernel-driver.sh
source setup.sh
export MYRTLE_LICENSE=&lt;your-license-file&gt;
</code></pre>
</li>
<li>
<p>Compile the C runtime example:</p>
<pre><code class="language-sh">(cd example; make)
</code></pre>
</li>
<li>
<p>Run the example.</p>
<p>For a block-size 64 accelerator such as <code>vollo-ia840f-c3b64.jic</code>:</p>
<pre><code class="language-sh">./example/vollo-example example/identity_b64.vollo
</code></pre>
<p>For a block-size 32 accelerator such as <code>vollo-ia420f-c6b32.jic</code>:</p>
<pre><code class="language-sh">./example/vollo-example example/identity_b32.vollo
</code></pre>
<p>You should see an output similar to the following:</p>
<pre><code class="language-sh">Using program: "example/identity_b64.vollo"
Using vollo-rt version: 18.0.0
Using Vollo accelerator with 3 core(s) and block_size 64
Program metadata for model 0:
  1 input with shape: [128]
  1 output with shape: [128]
Starting 10000 inferences
Done
Ran 10000 inferences in 0.020185 s with:
  mean latency of 2.004259 us
  99% latency of 2.176000 us
  throughput of 495411.228723 inf/s
</code></pre>
</li>
</ol>
<div style="break-before: page; page-break-before: always;"></div><h1 id="running-the-benchmark"><a class="header" href="#running-the-benchmark">Running the benchmark</a></h1>
<p>The release comes with a benchmark script that can be used to measure the
performance of the accelerator for a variety of models.
The script uses the vollo compiler to compile the models for your accelerator and then runs the
models on the accelerator to measure the performance.</p>
<ol>
<li>
<p>Install the script dependencies:</p>
<pre><code class="language-bash">sudo apt install python3-venv jq
</code></pre>
<p>Note, the compiler requires python 3.7 or later.</p>
</li>
<li>
<p>Ensure you have run the setup steps:</p>
<pre><code class="language-sh">cd &lt;vollo-sdk&gt;
sudo ./load_kernel_driver.sh
source setup.sh
export MYRTLE_LICENSE=&lt;your-license-file&gt;
</code></pre>
</li>
<li>
<p>Run the benchmark:</p>
<pre><code class="language-sh">$VOLLO_SDK/example/benchmark.sh
</code></pre>
</li>
<li>
<p>You can cross reference your numbers with those in the <a href="benchmark.html">benchmarks</a> section of the documentation.</p>
</li>
</ol>
<div style="break-before: page; page-break-before: always;"></div><h1 id="vollo-runtime"><a class="header" href="#vollo-runtime">Vollo Runtime</a></h1>
<p>The Vollo runtime provides a low latency asynchronous inference API for timing
critical inference requests on the Vollo accelerator.</p>
<p>A couple of example C programs that use the Vollo runtime API have been included in the
installation in the <code>example/</code> directory.</p>
<p>In order to use the Vollo runtime you need to have <a href="./accelerator-setup.html">an accelerator set up</a>:</p>
<ul>
<li><a href="./programming-the-fpga.html">A programmed FPGA</a></li>
<li><a href="./licensing.html">A loaded kernel driver and an installed license</a></li>
<li>Environment set up with <code>source setup.sh</code></li>
</ul>
<h2 id="python-api"><a class="header" href="#python-api">Python API</a></h2>
<p>The Vollo SDK includes Python bindings for the Vollo runtime. These can be more
convenient than the C API for e.g. testing Vollo against PyTorch models.</p>
<!-- markdown-link-check-disable -->
<p>The API for the Python bindings can be found <a href="./api-reference/vollo_rt.html">here</a>.</p>
<!-- markdown-link-check-enable -->
<p>A small example of using the Python bindings is provided <a href="./vollo-rt-python-example.html">here</a>.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="c-api"><a class="header" href="#c-api">C API</a></h1>
<p>The Vollo runtime API is a C API with simple types and functions in order to be
straight forward to use from any language with a C FFI.</p>
<ul>
<li>Header file: <code>$VOLLO_SDK/include/vollo-rt.h</code></li>
<li>Dynamic library: <code>$VOLLO_SDK/lib/libvollo_rt.so</code></li>
<li>Static library: <code>$VOLLO_SDK/lib/libvollo_rt.a</code></li>
</ul>
<p>It was built against GLIBC version 2.17.</p>
<p>To compile against Vollo RT with a standard C compiler, you can use the following flags:
<code>-I $VOLLO_SDK/include -L $VOLLO_SDK/lib -lvollo_rt</code></p>
<p>These are the main steps (in order) a program using <code>vollo_rt</code> will follow:</p>
<ol>
<li>Initialise the Vollo runtime using <code>vollo_rt_init</code></li>
<li>Add Vollo accelerators to the runtime using <code>vollo_rt_add_accelerator</code>
(Note: the current release only supports one accelerator)</li>
<li>Load a Vollo program onto the Vollo accelerators with <code>vollo_rt_load_program</code></li>
<li>Optionally, inspect the metadata about the models in the program using API calls such as
<code>vollo_rt_num_models</code> and <code>vollo_rt_model_num_inputs</code></li>
<li>Queue and run inference jobs by first calling <code>vollo_rt_add_job_bf16</code> (or
<code>vollo_rt_add_job_fp32</code>) and then polling in a loop for their completion using <code>vollo_rt_poll</code>.
You can queue several jobs before calling <code>vollo_rt_poll</code> or add extra jobs at any point.</li>
<li>Finally call <code>vollo_rt_destroy</code> to release resources.</li>
</ol>
<p>The API is designed to explicitly return errors when it can to let the user handle them as they see fit.
The metadata functions will instead error out themselves if any of the documented pre-conditions they rely on aren't met.
Any other crash is considered a bug and we would be very grateful if you could tell us about it.</p>
<h2 id="initialisation"><a class="header" href="#initialisation">Initialisation</a></h2>
<p>A vollo context is created by calling <code>vollo_rt_init</code>.
Add an accelerator by using the <code>vollo_rt_add_accelerator</code> function.</p>
<pre><code class="language-c">/**
 * Initialise the vollo-rt context. This must be called before any other vollo-rt functions.
 *
 * Logging level can be configured by setting the environment variable `VOLLO_RT_LOG` to one of:
 * "error", "warn", "info", "debug", or "trace"
 */
vollo_rt_error_t vollo_rt_init(vollo_rt_context_t* context_ptr);

/**
 * Destroy vollo-rt context, releasing its associated resources.
 */
void vollo_rt_destroy(vollo_rt_context_t vollo);

/**
 * Add an accelerator.
 * The accelerator is specified by its index. The index refers to an accelerator in the sorted list
 * of PCI addresses. This should be called after `vollo_rt_init` but before `vollo_rt_load_program`
 */
vollo_rt_error_t vollo_rt_add_accelerator(vollo_rt_context_t vollo, size_t accelerator_index);
</code></pre>
<h2 id="loading-a-program"><a class="header" href="#loading-a-program">Loading a program</a></h2>
<p>A program is loaded onto the Vollo accelerator using the <code>vollo_rt_load_program</code> function.</p>
<pre><code class="language-c">/**
 * Load a program onto the Vollo accelerators.
 * This should be called after `vollo_rt_add_accelerator`
 *
 * A Vollo program is generated by the Vollo compiler, it is typically named
 * "&lt;program_name&gt;.vollo".
 * The program is intended for a specific hardware config (number of accelerators,
 * cores and other configuration options), this function will return an
 * error if any accelerator configuration is incompatible with the program.
 * Once loaded, the program provides inference for several models concurrently.
 *
 * Note: This should only be called once per `vollo_rt_context_t`, as such if
 * a program needs to be changed or reset, first `vollo_rt_destroy` the current
 * context, then start a new context with `vollo_rt_init`.
 */
vollo_rt_error_t vollo_rt_load_program(vollo_rt_context_t vollo, const char* program_path);
</code></pre>
<h2 id="model-metadata"><a class="header" href="#model-metadata">Model metadata</a></h2>
<p>Once a program is loaded, it provides inference for one or more models. Metadata about
a model is obtained with <code>vollo_rt_model_*</code> functions.</p>
<p>Each model can have multiple distinct inputs and outputs. Each input and each output has
a multi-dimensional shape associated with it. All of the metadata is defined by the program
as supplied by the Vollo compiler. All the shapes are statically defined.</p>
<p>Some models can be compiled as streaming statefully over a dimension, that dimension is then
erased from the inference shape but its possition can be recovered in the model metadata.</p>
<pre><code class="language-c">/**
 * Inspect the number of models in the program loaded onto the vollo.
 *
 * Programs can contain multiple models, a `model_index` is used to select a
 * specific model
 */
size_t vollo_rt_num_models(vollo_rt_context_t vollo);

/**
 * Get the number of inputs of a model
 *
 * Each input has its own distinct shape
 *
 * Requirements (panics otherwise):
 * - a program was loaded with `vollo_rt_load_program`
 * - `model_index &lt; vollo_rt_num_models`
 */
size_t vollo_rt_model_num_inputs(vollo_rt_context_t vollo, size_t model_index);

/**
 * Get the number of outputs of a model
 *
 * Each output has its own distinct shape
 *
 * Requirements (panics otherwise):
 * - a program was loaded with `vollo_rt_load_program`
 * - `model_index &lt; vollo_rt_num_models`
 */
size_t vollo_rt_model_num_outputs(vollo_rt_context_t vollo, size_t model_index);

/**
 * Get the shape for input at a given index
 *
 * The return value is a 0 terminated array of dims containing the input shape
 * The value lives for as long as the model
 *
 * Requirements (panics otherwise):
 * - a program was loaded with `vollo_rt_load_program`
 * - `model_index &lt; vollo_rt_num_models`
 * - `input_index &lt; vollo_rt_model_num_inputs`
 */
const size_t* vollo_rt_model_input_shape(
  vollo_rt_context_t vollo, size_t model_index, size_t input_index);

/**
 * Get the shape for output at a given index
 *
 * The return value is a 0 terminated array of dims containing the output shape
 * The value lives for as long as the model
 *
 * Requirements (panics otherwise):
 * - a program was loaded with `vollo_rt_load_program`
 * - `model_index &lt; vollo_rt_num_models`
 * - `output_index &lt; vollo_rt_model_num_outputs`
 */
const size_t* vollo_rt_model_output_shape(
  vollo_rt_context_t vollo, size_t model_index, size_t output_index);

/**
 * Get the number of elements for input at a given index
 *
 * This is simply the product of the dimensions returned by `vollo_rt_model_input_shape`,
 * it is provided to make it easier to allocate the correct number of elements.
 *
 * Requirements (panics otherwise):
 * - a program was loaded with `vollo_rt_load_program`
 * - `model_index &lt; vollo_rt_num_models`
 * - `input_index &lt; vollo_rt_model_num_inputs`
 */
size_t vollo_rt_model_input_num_elements(
  vollo_rt_context_t vollo, size_t model_index, size_t input_index);

/**
 * Get the number of elements for output at a given index
 *
 * This is simply the product of the dimensions returned by `vollo_rt_model_output_shape`,
 * it is provided to make it easier to allocate the correct number of elements.
 *
 * Requirements (panics otherwise):
 * - a program was loaded with `vollo_rt_load_program`
 * - `model_index &lt; vollo_rt_num_models`
 * - `output_index &lt; vollo_rt_model_num_outputs`
 */
size_t vollo_rt_model_output_num_elements(
  vollo_rt_context_t vollo, size_t model_index, size_t output_index);

/**
 * In a streaming model, the streaming dimension is not part of the shape.
 *
 * - It returns -1 when there is no streaming dimension
 * - It otherwise returns the dim index
 *   For example, for a shape `(a, b, c)` and streaming dim index 1, the full shape is:
 *   `(a, streaming_dim, b, c)`
 *
 * Requirements (panics otherwise):
 * - a program was loaded with `vollo_rt_load_program`
 * - `model_index &lt; vollo_rt_num_models`
 * - `input_index &lt; vollo_rt_model_num_inputs`
 */
int vollo_rt_model_input_streaming_dim(
  vollo_rt_context_t vollo, size_t model_index, size_t input_index);

/**
 * In a streaming model, the streaming dimension is not part of the shape.
 *
 * - It returns -1 when there is no streaming dimension
 * - It otherwise returns the dim index
 *   For example, for a shape `(a, b, c)` and streaming dim index 1, the full shape is:
 *   `(a, streaming_dim, b, c)`
 *
 * Requirements (panics otherwise):
 * - a program was loaded with `vollo_rt_load_program`
 * - `model_index &lt; vollo_rt_num_models`
 * - `output_index &lt; vollo_rt_model_num_outputs`
 */
int vollo_rt_model_output_streaming_dim(
  vollo_rt_context_t vollo, size_t model_index, size_t output_index);
</code></pre>
<h2 id="running-inference"><a class="header" href="#running-inference">Running inference</a></h2>
<p>The interface returns results asynchronously so that inference requests can be made as fast
as the system can support, without blocking on output data being returned. This way, it also
supports running multiple requests concurrently.
Before any compute is started a job with associated input and output buffers needs to be
registered with the runtime using one of <code>vollo_rt_add_job_bf16</code> or <code>vollo_rt_add_job_fp32</code>.</p>
<p>The <code>bf16</code> variant uses <a href="https://cloud.google.com/tpu/docs/bfloat16"><code>bfloat16</code></a>
which is effectively a cropped version of single precision floating point format
<code>fp32</code> (same exponent, smaller mantissa).
Note: do NOT use C floating point literals for <code>bf16</code> as it is simply a <code>uint16_t</code> in the API</p>
<p>A <code>fp32</code> variant is also provided despite the Vollo accelerator expecting its
inputs and outputs to be in <code>fp16</code>. If you are working with <code>fp32</code>, prefer
this version instead of the <code>bf16</code> variant as it is able to make the conversion
while copying to/from DMA buffers, avoiding an extra copy.</p>
<pre><code class="language-c">/**
 * Sets up a computation on the vollo accelerator where the inputs and outputs are in brain-float 16
 * format.
 *
 * Note: The computation is only started on the next call to vollo_rt_poll. This way it is possible
 * to set up several computations that are kicked off at the same time.
 *
 * - vollo:
 *     the context that the computation should be run on
 * - model_index:
 *     the model to run
 * - user_ctx:
 *     a user context that will be returned on completion. This can be used to disambiguate when
 *     multiple models are running concurrently.
 *     NOTE: the jobs for a single model are guaranteed to come back in order, but the jobs for
 *     different models are not.
 * - input_data:
 *     a pointer to the start of an array with pointers to the start of the data to each input the
 *     number of inputs is given by `vollo_rt_model_num_inputs` each input length is the product of
 *     the shape given by `vollo_rt_model_input_shape`
 *     (or more convenient: `vollo_rt_model_input_num_elements`)
 *     lifetime:
 *       - The outer array only needs to live until `vollo_rt_add_job_bf16` returns
 *       - The input buffers need to live until `vollo_rt_poll` returns with the completion for
 *         this job
 * - output_data:
 *     a pointer to the start of an array with pointers to the start of the data to each output
 *     buffer the number of outputs is given by `vollo_rt_model_num_outputs` each output length is
 *     the product of the shape given by `vollo_rt_model_output_shape`
 *     (or more convenient: `vollo_rt_model_output_num_elements`)
 *     lifetime:
 *       - The outer array only needs to live until `vollo_rt_add_job_bf16` returns
 *       - The output buffers need to live until `vollo_rt_poll` returns with the completion for
 *         this job
 */
vollo_rt_error_t vollo_rt_add_job_bf16(
  vollo_rt_context_t vollo,
  size_t model_index,
  uint64_t user_ctx,
  const bf16* const* input_data,
  bf16* const* output_data);

vollo_rt_error_t vollo_rt_add_job_fp32(
  vollo_rt_context_t vollo,
  size_t model_index,
  uint64_t user_ctx,
  const float* const* input_data,
  float* const* output_data);
</code></pre>
<p>To actually start and later complete an inference you must use the <code>vollo_rt_poll</code> function
multiple times. It is typically called in a loop (with a timeout) until some or all of the jobs
are completed.</p>
<pre><code class="language-c">/**
 * Poll the vollo accelerator for completion.
 *
 * Note: Polling also initiates transfers for new jobs, so poll must be called
 * before any progress on these new jobs can be made.
 *
 *   num_completed: out: the number of completed user_ctx returned
 *   returned_user_ctx: buffer for the returned user_ctx of completed jobs, this will only be
 *                      valid until the next call to vollo_rt_poll.
 */
vollo_rt_error_t vollo_rt_poll(
  vollo_rt_context_t vollo, size_t* num_completed, const uint64_t** returned_user_ctx);
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="vollo-rt-example"><a class="header" href="#vollo-rt-example">Vollo RT Example</a></h1>
<p>The full code for this example can be found in <code>example/identity.c</code>.</p>
<p>Here we will work through it step by step.</p>
<hr />
<p>First we need to get hold of a Vollo RT context:</p>
<pre><code class="language-c">//////////////////////////////////////////////////
// Init
vollo_rt_context_t ctx;
EXIT_ON_ERROR(vollo_rt_init(&amp;ctx));
</code></pre>
<p>Note: throughout this example we use <code>EXIT_ON_ERROR</code>, it is just a convenient way to handle errors</p>
<hr />
<p>Then we need to add accelerators, the <code>accelerator_index</code> refers to the index of
the Vollo accelerator in the sorted list of PCI addresses, simply use <code>0</code> if you
have a single accelerator, or just want to use the first one.</p>
<pre><code class="language-c">//////////////////////////////////////////////////
// Add accelerators
size_t accelerator_index = 0;
EXIT_ON_ERROR(vollo_rt_add_accelerator(ctx, accelerator_index));
</code></pre>
<p>This step will check the accelerator license and make sure the bitstream is the
correct version and compatible with this version of the runtime.</p>
<hr />
<p>Then we load a program:</p>
<pre><code class="language-c">//////////////////////////////////////////////////
// Load program

// Program for a block_size 64 accelerator
const char* vollo_program_path = "./identity_b64.vollo";
EXIT_ON_ERROR(vollo_rt_load_program(ctx, vollo_program_path));
</code></pre>
<p>Here we're using a relative path (in the <code>example</code> directory) to one of the
example Vollo program, a program that computes the identity function for a tensor of size 128. The program
is specifically for a block_size 64 version of the accelerator such as the
default configuration for the <code>IA840F</code> FPGA.</p>
<hr />
<p>Then we setup some inputs and outputs for a single inference:</p>
<pre><code class="language-c">//////////////////////////////////////////////////
// Setup inputs and outputs

size_t model_index = 0;

// Assert model only has a single input and a single output tensor
assert(vollo_rt_model_num_inputs(ctx, model_index) == 1);
assert(vollo_rt_model_num_outputs(ctx, model_index) == 1);

assert(vollo_rt_model_input_num_elements(ctx, model_index, 0) == 128);
assert(vollo_rt_model_output_num_elements(ctx, model_index, 0) == 128);

float input_tensor[128];
float output_tensor[128];

for (size_t i = 0; i &lt; 128; i++) {
  input_tensor[i] = 42.0;
}
</code></pre>
<p>We check that the program metadata matches our expectations and we setup an input and output buffer.</p>
<hr />
<p>Then we run a single inference:</p>
<pre><code class="language-c">//////////////////////////////////////////////////
// Run an inference

single_shot_inference(ctx, input_tensor, output_tensor);
</code></pre>
<p>Where we define a convenience function to run this type of simple synchronous
inference on top of the asynchronous Vollo RT API:</p>
<pre><code class="language-c">// A small wrapper around the asynchronous Vollo RT API to block on a single inference
// This assume a single model with a single input and output tensor
static void single_shot_inference(vollo_rt_context_t ctx, const float* input, float* output) {
  size_t model_index = 0;

  const float* inputs[1] = {input};
  float* outputs[1] = {output};

  // user_ctx is not needed when doing single shot inferences
  // it can be used when doing multiple jobs concurrently to keep track of which jobs completed
  uint64_t user_ctx = 0;

  // Register a new job
  EXIT_ON_ERROR(vollo_rt_add_job_fp32(ctx, model_index, user_ctx, inputs, outputs));

  // Poll until completion
  size_t num_completed = 0;
  const uint64_t* completed_buffer = NULL;
  size_t poll_count = 0;

  while (num_completed == 0) {
    EXIT_ON_ERROR(vollo_rt_poll(ctx, &amp;num_completed, &amp;completed_buffer));

    poll_count++;
    if (poll_count &gt; 1000000) {
      EXIT_ON_ERROR("Timed out while polling");
    }
  }
}
</code></pre>
<p>This function does 2 things. First it registers a new job with the Vollo RT
context and then it polls in a loop until that job is complete.</p>
<p>For a more thorough overview of how to use this asynchronous API to run multiple
jobs concurrently take a look at <code>example/example.c</code></p>
<hr />
<p>And finally we print out the newly obtained results and cleanup the Vollo RT context:</p>
<pre><code class="language-c">//////////////////////////////////////////////////
// Print outputs

printf("Output values: [");
for (size_t i = 0; i &lt; 128; i++) {
  if (i % 8 == 0) {
    printf("\n  ");
  }

  printf("%.1f, ", output_tensor[i]);
}
printf("\n]\n");

//////////////////////////////////////////////////
// Release resources / Cleanup
vollo_rt_destroy(ctx);
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="vollo-rt-python-example"><a class="header" href="#vollo-rt-python-example">Vollo RT Python Example</a></h1>
<p>The Vollo RT Python bindings are provided for convenience, the runtime
performance of this API is not a priority.</p>
<!-- markdown-link-check-disable -->
<p>Here is a minimal way to use the <a href="./api-reference/vollo_rt.html">Vollo RT Python bindings</a>:</p>
<!-- markdown-link-check-enable -->
<pre><code class="language-python">import vollo_rt
import torch
import os

with vollo_rt.VolloRTContext() as ctx:
    ctx.add_accelerator(0)

    if ctx.accelerator_block_size(0) == 32:
        ctx.load_program(f"{os.environ["VOLLO_SDK"]}/example/identity_b32.vollo")
    else:
        ctx.load_program(f"{os.environ["VOLLO_SDK"]}/example/identity_b64.vollo")

    input = torch.rand(*ctx.model_input_shape()).bfloat16()
    output = ctx.run(input)

    torch.testing.assert_close(input, output)
    print("Success!")
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="vollo-ip-core"><a class="header" href="#vollo-ip-core">Vollo IP Core</a></h1>
<p>The Vollo IP Core is an encrypted netlist for Agilex FPGAs, that can be integrated into your own FPGA design.
It is available upon request, and can be configured by number of cores and the core size.</p>
<p>An IP Core release consists of the following components:</p>
<ul>
<li>Vollo IP Core encrypted netlist</li>
<li>Vollo Config C API for programming and activating the IP Core</li>
<li>Example design for the IP Core</li>
<li>Compiler configuration file for the IP Core, which can be used by the compiler to generate programs for the particular IP Core configuration</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="selecting-an-ip-core-configuration"><a class="header" href="#selecting-an-ip-core-configuration">Selecting an IP core configuration</a></h1>
<p>The Vollo IP core is scalable by the number of cores and the size of each core. To identify the best configuration for
your application, we advise the following steps:</p>
<ol>
<li>
<p>Test your ML models by running them through the Vollo compiler with different configurations. The virtual machine
will give you performance metrics for each configuration.</p>
</li>
<li>
<p>Reference the resource usage table below to understand the resource requirements for each configuration.</p>
</li>
</ol>
<p>Note, for initial integration and testing, it is recommended to start with a single core and a small block size. This will
allow for quicker iteration and debugging of the system.</p>
<h2 id="compiler-support"><a class="header" href="#compiler-support">Compiler support</a></h2>
<p>To test an IP Core configuration with the Vollo compiler, you can create the configuration in python as follows:</p>
<pre><code class="language-python">import vollo_compiler
ip_core_config = vollo_compiler.Config.ip_core(num_cores = 1, block_size = 32)
</code></pre>
<p>Once you have received your Vollo IP core, it comes bundled with a Vollo accelerator configuration file in JSON format,
in <code>vollo_ip_core/vollo-ip-core-config.json</code>.
This can be loaded into an accelerator configuration in python as follows:</p>
<pre><code class="language-python"><span class="boring">import os
</span><span class="boring">os.mkdir("vollo_ip_core")
</span><span class="boring">vollo_compiler.Config.save(vollo_compiler.Config.ip_core(num_cores = 1, block_size = 32), "vollo_ip_core/vollo-ip-core-config.json")
</span>ip_core_config = vollo_compiler.Config.load("vollo_ip_core/vollo-ip-core-config.json")
<span class="boring">os.remove("vollo_ip_core/vollo-ip-core-config.json")
</span><span class="boring">os.rmdir("vollo_ip_core")
</span></code></pre>
<p>From here, Vollo programs can be generated using the same workflow as described in <a href="ip-core/../example-1-mlp.html">example 1</a>
using the config imported above in the call to <code>to_program</code>:</p>
<pre><code class="language-python"><span class="boring">import torch
</span><span class="boring">import torch.nn as nn
</span><span class="boring">import torch.nn.functional as F
</span><span class="boring">import vollo_torch
</span><span class="boring">
</span><span class="boring">class MLP(nn.Module):
</span><span class="boring">    def __init__(self, input_size, output_size, hidden_size):
</span><span class="boring">        super().__init__()
</span><span class="boring">        self.fc1 = nn.Linear(input_size, hidden_size)
</span><span class="boring">        self.fc2 = nn.Linear(hidden_size, hidden_size)
</span><span class="boring">        self.out = nn.Linear(hidden_size, output_size)
</span><span class="boring">
</span><span class="boring">    def forward(self, x):
</span><span class="boring">        x = F.relu(self.fc1(x))
</span><span class="boring">        residual = x
</span><span class="boring">        x = F.relu(self.fc2(x)) + residual
</span><span class="boring">        return self.out(x)
</span><span class="boring">
</span># Make a pytorch model
input_size = 784
output_size = 10
hidden_size = 128
model = MLP(input_size, output_size, hidden_size)

# Annotate the model with activation shapes
input = torch.randn(input_size)
(model, expected_output) = vollo_torch.fx.prepare_shape(model, input)

# Compile to a Vollo program using the loaded ip_core_config
nnir = vollo_torch.fx.nnir.to_nnir(model)
program = nnir.to_program(ip_core_config)

</code></pre>
<p>Run the VM to get the cycle count:</p>
<pre><code class="language-python">vm = program.to_vm()
vm_output = vm.run(input.detach().numpy())
torch.testing.assert_close(expected_output, torch.from_numpy(vm_output), atol = 1e-2, rtol=1e-2)
print("cycle count:", vm.cycle_count())
</code></pre>
<h2 id="resource-usage"><a class="header" href="#resource-usage">Resource usage</a></h2>
<p>The following table shows the resource usage of the Vollo IP Core for different configurations. Note,
these resources may vary depending on the Vollo SDK version.</p>
<p>The block size determines the side of the matrix block. The core scales with the square of this parameter
e.g. a block size 64 core is around 4 times larger than a block size 32 core.</p>
<div class="table-wrapper"><table><thead><tr><th>Cores</th><th>Block size</th><th>ALMs</th><th>M20Ks</th><th>DSPs</th></tr></thead><tbody>
<tr><td>1</td><td>32</td><td>43K</td><td>1084</td><td>624</td></tr>
<tr><td>2</td><td>32</td><td>78K</td><td>2000</td><td>1248</td></tr>
<tr><td>3</td><td>32</td><td>115K</td><td>2932</td><td>1872</td></tr>
<tr><td>4</td><td>32</td><td>152K</td><td>3880</td><td>2496</td></tr>
<tr><td>5</td><td>32</td><td>194K</td><td>4844</td><td>3120</td></tr>
<tr><td>6</td><td>32</td><td>231K</td><td>5824</td><td>3744</td></tr>
<tr><td>1</td><td>64</td><td>106K</td><td>3065</td><td>2400</td></tr>
<tr><td>2</td><td>64</td><td>207K</td><td>5840</td><td>4800</td></tr>
<tr><td>3</td><td>64</td><td>308K</td><td>8631</td><td>7200</td></tr>
</tbody></table>
</div><div style="break-before: page; page-break-before: always;"></div><h1 id="ip-core-interface"><a class="header" href="#ip-core-interface">IP Core Interface</a></h1>
<p>The Vollo IP Core is programmed with a neural network model (Vollo program) via a configuration bus. Once this is done, the IP Core
can run the model by streaming input data to the IP Core and receiving the output data.</p>
<p><img src="ip-core/../assets/vollo-ip-core.svg" alt="Vollo IP Core" /></p>
<p>The IP Core has the following interfaces:</p>
<ul>
<li>Config clock and reset signals. This a clock which is expected to be at a frequency of around 100MHz. It is only used for
configuration and not for running the model.</li>
<li>Config bus. This is a 32-bit wide AXI4-Lite bus used to activate the device with a license key and to configure the IP Core. It is synchronous to the config clock.</li>
<li>Compute clock and reset signals. This is the clock used for running the model. In the example design this clock frequency is
set to 320MHz.</li>
<li>Input data bus. This a AXI4-Stream bus used to stream input data to the IP Core. It size varies depending on the size of the
cores in the IP. For a 32-block size design, this is 512 wide (16 bits per value using brainfloat 16). It is synchronous to the compute clock.</li>
<li>Model selection bus. This is an AXI4-Stream interface for providing the model index to be run if the IP core has been configured
with multiple models. If the IP Core has been configured with a single model, providing the index is optional. It is synchronous to the compute clock.</li>
<li>Output data bus. This a AXI4-Stream bus used to stream output data from the IP Core. It is synchronous to the compute clock.</li>
</ul>
<h2 id="configuration-bus"><a class="header" href="#configuration-bus">Configuration bus</a></h2>
<p>The configuration bus is a 32-bit wide AXI4-Lite bus. The normal rules for AXI4-Lite buses
should be followed with the following exceptions:</p>
<ul>
<li>Write strobe: The write strobe should either be fully asserted or fully deasserted. Partially asserted write strobes are not supported.</li>
<li>The protection signals, <code>config_awprot</code> and <code>config_arprot</code>, are unused and ignored.</li>
</ul>
<p>Verilog signals:</p>
<pre><code class="language-verilog">    // Config interface clock and active-high synchronous reset:
      input  logic          config_clock
    , input  logic          config_reset

    // Config AXI4-Lite interface.
    // The config_awprot and config_arprot inputs are unused
    // and ignored.
    , input  logic          config_awvalid
    , output logic          config_awready
    , input  logic [20:0]   config_awaddr
    , input  logic [2:0]    config_awprot

    , input  logic          config_wvalid
    , output logic          config_wready
    , input  logic [31:0]   config_wdata
    , input  logic [3:0]    config_wstrb

    , input  logic          config_arvalid
    , output logic          config_arready
    , input  logic [20:0]   config_araddr
    , input  logic [2:0]    config_arprot

    , output logic          config_rvalid
    , input  logic          config_rready
    , output logic [31:0]   config_rdata
    , output logic [1:0]    config_rresp

    , output logic          config_bvalid
    , input  logic          config_bready
    , output logic [1:0]    config_bresp

</code></pre>
<p>The AXI4-Lite interface is used to configure the Vollo IP Core and to do so must be accessible from
the host system. The configuration can then be done by providing functions to communicate with the
bus to <a href="ip-core/4-config.html">Vollo configuration API</a>.</p>
<h2 id="input-and-output-streams"><a class="header" href="#input-and-output-streams">Input and Output Streams</a></h2>
<p>The input and output streams are AXI4-Stream interfaces. The input and output are packed as flattened tensor and
padded to the next multiple of block-size. The data should be packed in <em>little-endian</em> format. The output stream
includes <code>tkeep</code> and <code>tlast</code> signals to indicate when the end of the packet and which bytes are valid (i.e. not padding).</p>
<p>For example, an input of tensor dimension <code>[62]</code> to an ip-core with block size 32 should be provided as two
words, the first with a full 32 brainfloat values, and the second with the remaining 30 brainfloat values and 2 padding values.
They should be packed as follows:</p>
<div class="table-wrapper"><table><thead><tr><th>Word</th><th>511:496</th><th>495:480</th><th>479:464</th><th>...</th><th>31:16</th><th>15:0</th></tr></thead><tbody>
<tr><td>0</td><td><code>input[31]</code></td><td><code>input[30]</code></td><td><code>input[29]</code></td><td>...</td><td><code>input[1]</code></td><td><code>input[0]</code></td></tr>
<tr><td>1</td><td><code>X</code></td><td><code>X</code></td><td><code>input[61]</code></td><td>...</td><td><code>input[33]</code></td><td><code>input[32]</code></td></tr>
</tbody></table>
</div>
<p>Verilog signals:</p>
<pre><code class="language-verilog">
    // Core clock and active-high synchronous reset:
    , input  logic          compute_clock
    , input  logic          compute_reset

    // Input AXI4-Stream interface:
    , input  logic          input_tvalid
    , output logic          input_tready
    , input  logic [511:0]  input_tdata

    // Output AXI4-Stream interface:
    , output logic          output_tvalid
    , input  logic          output_tready
    , output logic          output_tlast
    , output logic [63:0]   output_tkeep
    , output logic [511:0]  output_tdat

</code></pre>
<h2 id="model-selection"><a class="header" href="#model-selection">Model Selection</a></h2>
<blockquote>
<p>⚠️ The IP Core does not currently support multiple models. This feature is planned for a future release.
The model selection bus is ignored and can be driven with any value.</p>
</blockquote>
<p>The model selection bus can be used to select between multiple models that have been configured into the IP Core. This selection
can be provided ahead of the data stream. The model selection bus is a 16-bit wide AXI4-Stream interface.</p>
<p>For single model Vollo programs, it is not required to drive the model selection bus, however the IP Core
will accept the value if it is driven.</p>
<pre><code class="language-verilog">
    // Model select AXI4-Stream interface:
    , input  logic         model_select_tvalid
    , output logic         model_select_tready
    , input  logic [15:0]  model_select_tdata

</code></pre>
<h2 id="verilog-instantiation"><a class="header" href="#verilog-instantiation">Verilog instantiation</a></h2>
<p>The complete component interface is as follows:</p>
<pre><code class="language-sv">
module vollo_ip_core
  (
    // Config interface clock and active-high synchronous reset:
      input  logic          config_clock
    , input  logic          config_reset

    // Config AXI4-Lite interface.
    // The config_awprot and config_arprot inputs are unused
    // and ignored.
    , input  logic          config_awvalid
    , output logic          config_awready
    , input  logic [20:0]   config_awaddr
    , input  logic [2:0]    config_awprot

    , input  logic          config_wvalid
    , output logic          config_wready
    , input  logic [31:0]   config_wdata
    , input  logic [3:0]    config_wstrb

    , input  logic          config_arvalid
    , output logic          config_arready
    , input  logic [20:0]   config_araddr
    , input  logic [2:0]    config_arprot

    , output logic          config_rvalid
    , input  logic          config_rready
    , output logic [31:0]   config_rdata
    , output logic [1:0]    config_rresp

    , output logic          config_bvalid
    , input  logic          config_bready
    , output logic [1:0]    config_bresp

    // Core clock and active-high synchronous reset:
    , input  logic          compute_clock
    , input  logic          compute_reset

    // Model select AXI4-Stream interface:
    , input  logic          model_select_tvalid
    , output logic          model_select_tready
    , input  logic [15:0]   model_select_tdata

    // Input AXI4-Stream interface:
    , input  logic          input_tvalid
    , output logic          input_tready
    , input  logic [511:0]  input_tdata

    // Output AXI4-Stream interface:
    , output logic          output_tvalid
    , input  logic          output_tready
    , output logic          output_tlast
    , output logic [63:0]   output_tkeep
    , output logic [511:0]  output_tdata
  );

</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="quartus-integration"><a class="header" href="#quartus-integration">Quartus Integration</a></h1>
<h2 id="quartus-license-for-encrypted-rtl"><a class="header" href="#quartus-license-for-encrypted-rtl">Quartus license for Encrypted RTL</a></h2>
<p>Add the quartus license file, <code>vollo_ip_core/vollo-ip-quartus.lic</code>, to Quartus to decrypt the Vollo IP core. You can either:</p>
<ol>
<li>
<p>Add the license file to your license server.</p>
</li>
<li>
<p>Add the license file locally within Quartus by setting the environment variable <code>LM_LICENSE_FILE</code> or adding
the license file in the GUI <code>Tools &gt; License Setup</code>.</p>
</li>
</ol>
<h2 id="sourcing-design"><a class="header" href="#sourcing-design">Sourcing design</a></h2>
<p>The following files need sourcing:</p>
<ul>
<li><code>vollo_ip_core/vollo_ip_core.sv</code> the top-level interface.</li>
<li><code>vollo_ip_core/vollo_ip_core_internal.sv</code> the encrypted RTL design.</li>
<li>The memory initialization files, <code>.mif</code>, in <code>vollo_ip_core</code>.</li>
</ul>
<p>This can be done in the GUI or by using the following <code>tcl</code> commands:</p>
<pre><code class="language-tcl">require file_util

foreach file [fileutil::findByPattern vollo_ip_core *.sv] {
  set_global_assignment -name SYSTEM_VERILOG_FILE $file
}
foreach file [fileutil::findByPattern vollo_ip_core *.mif] {
  set_global_assignment -name MIF_FILE $file
}
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="runtime-configuration"><a class="header" href="#runtime-configuration">Runtime configuration</a></h1>
<p>The Vollo IP Core needs to be programmed with a Vollo program and have its license activated before it can run inferences.
This configuration happens over the AXI4-Lite configuration bus of the Vollo IP Core.</p>
<p>A C library is provided to do this configuration. In order to use the library the user must supply functions for
writing and reading to Vollo IP Core configuration bus. The library then has functions to activate the IP core using
a license and configure the core with a program.</p>
<p>The C API is provided as part of the <code>VOLLO_SDK</code>:</p>
<ul>
<li>Header is <code>$VOLLO_SDK/include/vollo-cfg.h</code></li>
<li>Library is <code>$VOLLO_SDK/lib/libvollo_cfg.so</code> (or <code>libvollo_cfg.a</code> for static linking)</li>
</ul>
<p>The key concept behind this API is that the user of the Vollo IP Core is in full control of the communication over the configuration bus.
Please refer to the documentation in the header file for details on how to use the API and which considerations should be taken when setting up communication with the configuration bus.</p>
<h2 id="licensing-a-vollo-ip-core"><a class="header" href="#licensing-a-vollo-ip-core">Licensing a Vollo IP Core</a></h2>
<p>On the first use of an FPGA with the Vollo IP Core, information about the device needs to be gathered in order to acquire a license.
The <a href="ip-core/../licensing.html">section about licensing</a> describes how to use <code>vollo-tool</code> to get the device ID information.
Unfortunately this method does not work for the Vollo IP Core, as <code>vollo-tool</code> does not know how to communicate with the Vollo IP Core.
The C API provides the function <code>vollo_cfg_print_device_id</code>, this prints the relevant information to STDOUT using the provided reader and writer to communicate with the IP Core.</p>
<p>Once a license has been acquired, the function <code>vollo_cfg_activate_license</code> is used to activate a license (it uses the <code>MYRTLE_LICENSE</code> environment variable to find the license file).</p>
<h2 id="example-design"><a class="header" href="#example-design">Example design</a></h2>
<p>The Vollo IP Core release contains an example runtime which makes use of this API: <code>vollo_ip_core_example/runtime/vollo_cfg_example.c</code></p>
<p>The example runtime is written to work with the <a href="ip-core/./5-example-design.html">example</a> RTL instantiation of the Vollo IP Core which sets up the configuration bus to be accessible over BAR 2 using the ifc_uio kernel driver
(the patched version of the kernel driver provided in <code>VOLLO_SDK</code> works just as well as the original Intel provided one for this example)</p>
<p>These are the main steps of the configuration are:</p>
<ul>
<li>Acquire and activate a license</li>
<li>Load a Vollo program onto the Vollo IP Core with <code>vollo_cfg_load_program</code></li>
</ul>
<p>All of these functions use the same API with a custom reader and writer to communicate with the configuration bus.
The reader and writer are quite generic and take a user controllable context in order to be implementable in most environments (do tell us if this API does not suit your needs).</p>
<p>To build and run the example runtime, please refer to: <code>vollo_ip_core_example/runtime/README.md</code></p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="example-design-1"><a class="header" href="#example-design-1">Example design</a></h1>
<p>The Vollo IP Core comes with a simple example design.</p>
<p>The example design is for the Bittware IA-420F device - but can be converted to a different device by adapting the relevant files.</p>
<p>The example design contains:</p>
<ul>
<li><code>rtl</code>:
a simple instantiation of <code>vollo_ip_core</code> with the input and output AXI4-Stream interfaces are
hooked up to directly MMIO.
An efficient design should instead control input and output directly from the FPGA, or if
control from a CPU host is needed then use a proper DMA instead.</li>
<li><code>bitstream</code>:
this is a pre-built bitstream of the provided RTL, it is provided to speed up testing of the runtime</li>
<li><code>runtime</code>:
the runtime showcases how to configure and activate the Vollo IP Core using the <code>vollo-cfg</code> library.
It also controls IO to do basic inferences
(this IO mechanism is only an example, it is <em>not</em> optimized for performance)</li>
</ul>
<h2 id="building-rtl"><a class="header" href="#building-rtl">Building RTL</a></h2>
<p>A script is provided to automate building the RTL: <code>vollo_ip_core_example/rtl/build.sh</code></p>
<p>Alternatively, you can perform the following steps manually:</p>
<ul>
<li>
<p>Run QSYS:
This example project is provided as a TCL generated QSYS project.
So we need to generate the top level QSYS:</p>
<pre><code class="language-bash">qsys-script --script=qsys/mcdma_sys.tcl --quartus-project=agilex
</code></pre>
<p>And generate RTL for that QSYS project:</p>
<pre><code class="language-bash">qsys-generate mcdma_sys.qsys --synthesis=VERILOG --parallel --quartus-project=agilex.qpf
</code></pre>
</li>
<li>
<p>Run Quartus:
This step can be done manually in the GUI or with the provided <code>flow.tcl</code>
(which runs <code>syn</code>, <code>fit</code>, <code>sta</code>, and <code>asm</code>):</p>
<pre><code class="language-bash">quartus_sh -t flow.tcl
</code></pre>
<p>In order to synthesise the Vollo IP Core which is encrypted, make sure to add the <code>vollo-ip-quartus.lic</code>
file to your license server, or simply add it to the <code>LM_LICENSE_FILE</code> environment variable
(see <a href="ip-core/3-quartus-integration.html">Quartus integration</a>).</p>
</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="versions"><a class="header" href="#versions">Versions</a></h1>
<h2 id="version-compatibility"><a class="header" href="#version-compatibility">Version Compatibility</a></h2>
<p>The Vollo SDK follows a semantic versioning scheme.
If the left-most non-zero component (major/minor/patch) of the version number
is unchanged between two releases of the Vollo SDK, it should be possible to
update to the newer version without modifying your existing code, e.g. updating
from 0.12.0 to 0.12.1.</p>
<p>Additionally, the FPGA bitstream is stable between patch releases, so you do
not have to reprogram the FPGA with an updated Vollo SDK bitstream unless the
minor or major version numbers have changed.</p>
<h2 id="documentation-for-previous-versions"><a class="header" href="#documentation-for-previous-versions">Documentation for Previous Versions</a></h2>
<p>Documentation for previous versions of the Vollo SDK can be found in this
listing:</p>
<!-- version-listing-anchor-start -->
<!-- version-listing-anchor-end --><div style="break-before: page; page-break-before: always;"></div><h1 id="release-notes"><a class="header" href="#release-notes">Release Notes</a></h1>
<h2 id="2100"><a class="header" href="#2100">21.0.0</a></h2>
<ul>
<li>Runtime/bitstream optimisation for small inputs (using MMIO instead of DMA)</li>
<li>Scheduling and architecture optimisations</li>
<li>Add <code>reset</code> subcommand to <code>vollo-tool</code></li>
<li>Support ReLU via <code>torch.relu</code></li>
</ul>
<h2 id="2003"><a class="header" href="#2003">20.0.3</a></h2>
<ul>
<li>Separate bitstreams from Vollo SDK</li>
<li>Add c2b64d hw config to support models up to 8M parameters (bitstream and compiler)</li>
<li>Improve compiler error messages</li>
<li>Fix example build</li>
</ul>
<h2 id="2002"><a class="header" href="#2002">20.0.2</a></h2>
<ul>
<li>Fix for incorrect <code>vollo_rt_accelerator_num_cores</code> introduced in 20.0.1</li>
</ul>
<h2 id="2001"><a class="header" href="#2001">20.0.1</a></h2>
<ul>
<li><code>vollo_rt_add_vm</code> to test the <code>vollo-rt</code> API without an accelerator</li>
<li><code>vollo_rt_load_program_from_buffer</code> and <code>vollo_compiler.Program.{save,load}_bytes</code></li>
<li>Add <code>vollo_torch.nn.RecurrentStateLSTM</code> for modelling streaming LSTM models across forward passes</li>
<li>Codegen fix for <code>vollo_torch.nn.Scan</code></li>
<li>Fix incorrect input validation for <code>torch.sum</code> layers</li>
<li>Change vollo-rt example to compile with older C compilers</li>
</ul>
<h2 id="2000"><a class="header" href="#2000">20.0.0</a></h2>
<ul>
<li>Add support for LayerNorm</li>
<li>Add support for RMSNorm</li>
<li>Add support for sqrt operations (<code>torch.sqrt</code> and <code>torch.rsqrt</code>)</li>
<li>Add support for summing over the data dimension</li>
<li>Add <code>cycle_count_per_inference</code> and <code>compute_duration_per_inference_us</code> Program methods</li>
<li>Add support for a wider range of torch arithmetic operation aliases</li>
</ul>
<h2 id="1923"><a class="header" href="#1923">19.2.3</a></h2>
<ul>
<li>Downgrade glibc dependency to support systems with glibc &gt;=2.17</li>
</ul>
<h2 id="1922"><a class="header" href="#1922">19.2.2</a></h2>
<ul>
<li>Add support for <code>torch.div</code>, <code>torch.Tensor.div</code></li>
<li>Fix compiler code generation bug for division</li>
</ul>
<h2 id="1921"><a class="header" href="#1921">19.2.1</a></h2>
<ul>
<li>Add support for scalars on the left of division</li>
<li>Add support for <code>Reciprocal</code> node in ONNX frontend</li>
</ul>
<h2 id="1920"><a class="header" href="#1920">19.2.0</a></h2>
<ul>
<li>Add support for division by non-constant tensors</li>
<li>Fix slicing in ONNX frontend</li>
</ul>
<h2 id="1911"><a class="header" href="#1911">19.1.1</a></h2>
<ul>
<li>Fix compiler bug in constant folding</li>
</ul>
<h2 id="1910"><a class="header" href="#1910">19.1.0</a></h2>
<ul>
<li>Add support for partial updates of input data on the accelerator</li>
<li>VM simulates Vollo accelerator bit-accurately: <code>bf16_precision</code> argument
renamed to <code>bit_accurate</code> and enabled by default</li>
<li><code>vollo-tool</code> includes license self-service</li>
<li>Performance improvements due to DMA optimization</li>
</ul>
<h2 id="1802"><a class="header" href="#1802">18.0.2</a></h2>
<ul>
<li>Add <code>optimize_transforms</code> option to the compiler to improve program schedule in some cases</li>
</ul>
<h2 id="1801"><a class="header" href="#1801">18.0.1</a></h2>
<ul>
<li>Add fallback to Vollo RT and vollo-tool for when AVX is not available</li>
</ul>
<h2 id="1800"><a class="header" href="#1800">18.0.0</a></h2>
<ul>
<li>Vollo RT support for using raw DMA buffers to skip IO copy</li>
<li>Vollo RT remove redundant/noisy warnings on error: it is the user's responsibility to check returned errors</li>
<li>Compiler optimization for Where nodes</li>
<li>Compiler scheduling optimizations</li>
<li>Vollo IP Core public documentation</li>
</ul>
<h2 id="0171"><a class="header" href="#0171">0.17.1</a></h2>
<ul>
<li>Fix vollo-tool compatibility with older bitstreams</li>
</ul>
<h2 id="0170"><a class="header" href="#0170">0.17.0</a></h2>
<ul>
<li>New DMA engine that reduces IO latencies by ~1.3us</li>
<li>Initial support for non-streaming LSTM</li>
</ul>
<h2 id="0160"><a class="header" href="#0160">0.16.0</a></h2>
<ul>
<li>Vollo IP Core now available on request</li>
<li>Add C library for configuring IP Core: <code>vollo-cfg</code></li>
<li>Support for slicing/concatenation in the middle of models</li>
<li>Support for BatchNorm nodes</li>
<li>Support for Scan/LSTMCell nodes</li>
<li>Add <code>--io-only</code> option to <code>vollo-onnx</code></li>
<li>Add <code>program-metadata</code> command to <code>vollo-tool</code></li>
<li>Fix compiler bug with transposing streaming dimension</li>
<li>Fix accelerator bug in initial state of streaming models</li>
</ul>
<h2 id="0150"><a class="header" href="#0150">0.15.0</a></h2>
<ul>
<li>Accelerator bug fix</li>
</ul>
<h2 id="0140"><a class="header" href="#0140">0.14.0</a></h2>
<ul>
<li>Support for filtering dropout layers</li>
<li>Instruction packing improvements</li>
<li>LSTM performance improvement</li>
<li>Improvements to weight sharing</li>
</ul>
<h2 id="0130"><a class="header" href="#0130">0.13.0</a></h2>
<ul>
<li>Support for multi-model programs</li>
<li>Provide Python bindings to Vollo RT: <code>vollo_rt</code></li>
<li>Improved support and error messages for tensor indexing in compiler</li>
<li>The unweave transform is now automatic</li>
</ul>
<h2 id="0122"><a class="header" href="#0122">0.12.2</a></h2>
<ul>
<li>Support for LSTM nodes in ONNX frontend</li>
<li>Support for squeezing, unsqueezing, reduce sum, using <code>unweave</code>
transformation</li>
<li>Improved error reporting in <code>vollo_torch</code> lowering to NNIR</li>
</ul>
<h2 id="0121"><a class="header" href="#0121">0.12.1</a></h2>
<ul>
<li><code>vollo-torch</code> fix type hints being incompatible with Python 3.7/3.8</li>
<li><code>vollo-rt.h</code> fix namespacing issue (<code>error_t</code> -&gt; <code>vollo_rt_error_t</code>)</li>
<li>Runtime optimisations</li>
<li>Added IO only benchmarks</li>
</ul>
<h2 id="0120"><a class="header" href="#0120">0.12.0</a></h2>
<ul>
<li>Initial support for ONNX models in compiler</li>
<li>Support for LSTM nodes</li>
<li>Improved error reporting in compiler</li>
<li>Compiler API changes</li>
<li>New runtime API with access to model metadata</li>
<li>HW optimisations (pointwise operations)</li>
<li>IA840F support</li>
</ul>
<h2 id="0101"><a class="header" href="#0101">0.10.1</a></h2>
<ul>
<li>Support for scalar (<code>int</code>, <code>float</code>) literals in pointwise operations in
<code>vollo-torch</code>.</li>
</ul>
<h2 id="0100"><a class="header" href="#0100">0.10.0</a></h2>
<ul>
<li>Architectural changes in bitstream to support compiler</li>
<li>Reduced latency from reduced core to core communication in the bitstream</li>
<li>Add general model compiler and VM simulation with Python bindings in
<code>vollo-python</code></li>
<li>Add PyTorch frontend to model compiler in <code>vollo-torch</code></li>
</ul>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->


                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">

            </nav>

        </div>




        <script type="text/javascript">
            window.playground_copyable = true;
        </script>


        <script src="elasticlunr.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="mark.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="searcher.js" type="text/javascript" charset="utf-8"></script>

        <script src="clipboard.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="highlight.js" type="text/javascript" charset="utf-8"></script>
        <script src="book.js" type="text/javascript" charset="utf-8"></script>

        <!-- Custom JS scripts -->

        <script type="text/javascript">
        window.addEventListener('load', function() {
            window.setTimeout(window.print, 100);
        });
        </script>

    </body>
</html>
